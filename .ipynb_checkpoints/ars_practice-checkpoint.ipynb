{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a recreation of the ARS (Augmented Random Search) reinforcement learning algorithm example workflow tutorial by Colin Skow in [this repository](https://github.com/colinskow/move37/blob/master/ars/ars.py). I want to get a keen feel of the mechanics of ARS so that I might be able to apply it to CARLA. I will be using the BipedalWalker-v3 environment from the gym package to test this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v3), EnvSpec(BipedalWalkerHardcore-v3), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0), EnvSpec(KellyCoinflip-v0), EnvSpec(KellyCoinflipGeneralized-v0), EnvSpec(FrozenLake-v0), EnvSpec(FrozenLake8x8-v0), EnvSpec(CliffWalking-v0), EnvSpec(NChain-v0), EnvSpec(Roulette-v0), EnvSpec(Taxi-v3), EnvSpec(GuessingGame-v0), EnvSpec(HotterColder-v0), EnvSpec(Reacher-v2), EnvSpec(Pusher-v2), EnvSpec(Thrower-v2), EnvSpec(Striker-v2), EnvSpec(InvertedPendulum-v2), EnvSpec(InvertedDoublePendulum-v2), EnvSpec(HalfCheetah-v2), EnvSpec(HalfCheetah-v3), EnvSpec(Hopper-v2), EnvSpec(Hopper-v3), EnvSpec(Swimmer-v2), EnvSpec(Swimmer-v3), EnvSpec(Walker2d-v2), EnvSpec(Walker2d-v3), EnvSpec(Ant-v2), EnvSpec(Ant-v3), EnvSpec(Humanoid-v2), EnvSpec(Humanoid-v3), EnvSpec(HumanoidStandup-v2), EnvSpec(FetchSlide-v1), EnvSpec(FetchPickAndPlace-v1), EnvSpec(FetchReach-v1), EnvSpec(FetchPush-v1), EnvSpec(HandReach-v0), EnvSpec(HandManipulateBlockRotateZ-v0), EnvSpec(HandManipulateBlockRotateZTouchSensors-v0), EnvSpec(HandManipulateBlockRotateZTouchSensors-v1), EnvSpec(HandManipulateBlockRotateParallel-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensors-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensors-v1), EnvSpec(HandManipulateBlockRotateXYZ-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensors-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensors-v1), EnvSpec(HandManipulateBlockFull-v0), EnvSpec(HandManipulateBlock-v0), EnvSpec(HandManipulateBlockTouchSensors-v0), EnvSpec(HandManipulateBlockTouchSensors-v1), EnvSpec(HandManipulateEggRotate-v0), EnvSpec(HandManipulateEggRotateTouchSensors-v0), EnvSpec(HandManipulateEggRotateTouchSensors-v1), EnvSpec(HandManipulateEggFull-v0), EnvSpec(HandManipulateEgg-v0), EnvSpec(HandManipulateEggTouchSensors-v0), EnvSpec(HandManipulateEggTouchSensors-v1), EnvSpec(HandManipulatePenRotate-v0), EnvSpec(HandManipulatePenRotateTouchSensors-v0), EnvSpec(HandManipulatePenRotateTouchSensors-v1), EnvSpec(HandManipulatePenFull-v0), EnvSpec(HandManipulatePen-v0), EnvSpec(HandManipulatePenTouchSensors-v0), EnvSpec(HandManipulatePenTouchSensors-v1), EnvSpec(FetchSlideDense-v1), EnvSpec(FetchPickAndPlaceDense-v1), EnvSpec(FetchReachDense-v1), EnvSpec(FetchPushDense-v1), EnvSpec(HandReachDense-v0), EnvSpec(HandManipulateBlockRotateZDense-v0), EnvSpec(HandManipulateBlockRotateZTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateZTouchSensorsDense-v1), EnvSpec(HandManipulateBlockRotateParallelDense-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensorsDense-v1), EnvSpec(HandManipulateBlockRotateXYZDense-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensorsDense-v1), EnvSpec(HandManipulateBlockFullDense-v0), EnvSpec(HandManipulateBlockDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v1), EnvSpec(HandManipulateEggRotateDense-v0), EnvSpec(HandManipulateEggRotateTouchSensorsDense-v0), EnvSpec(HandManipulateEggRotateTouchSensorsDense-v1), EnvSpec(HandManipulateEggFullDense-v0), EnvSpec(HandManipulateEggDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v1), EnvSpec(HandManipulatePenRotateDense-v0), EnvSpec(HandManipulatePenRotateTouchSensorsDense-v0), EnvSpec(HandManipulatePenRotateTouchSensorsDense-v1), EnvSpec(HandManipulatePenFullDense-v0), EnvSpec(HandManipulatePenDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v1), EnvSpec(Adventure-v0), EnvSpec(Adventure-v4), EnvSpec(AdventureDeterministic-v0), EnvSpec(AdventureDeterministic-v4), EnvSpec(AdventureNoFrameskip-v0), EnvSpec(AdventureNoFrameskip-v4), EnvSpec(Adventure-ram-v0), EnvSpec(Adventure-ram-v4), EnvSpec(Adventure-ramDeterministic-v0), EnvSpec(Adventure-ramDeterministic-v4), EnvSpec(Adventure-ramNoFrameskip-v0), EnvSpec(Adventure-ramNoFrameskip-v4), EnvSpec(AirRaid-v0), EnvSpec(AirRaid-v4), EnvSpec(AirRaidDeterministic-v0), EnvSpec(AirRaidDeterministic-v4), EnvSpec(AirRaidNoFrameskip-v0), EnvSpec(AirRaidNoFrameskip-v4), EnvSpec(AirRaid-ram-v0), EnvSpec(AirRaid-ram-v4), EnvSpec(AirRaid-ramDeterministic-v0), EnvSpec(AirRaid-ramDeterministic-v4), EnvSpec(AirRaid-ramNoFrameskip-v0), EnvSpec(AirRaid-ramNoFrameskip-v4), EnvSpec(Alien-v0), EnvSpec(Alien-v4), EnvSpec(AlienDeterministic-v0), EnvSpec(AlienDeterministic-v4), EnvSpec(AlienNoFrameskip-v0), EnvSpec(AlienNoFrameskip-v4), EnvSpec(Alien-ram-v0), EnvSpec(Alien-ram-v4), EnvSpec(Alien-ramDeterministic-v0), EnvSpec(Alien-ramDeterministic-v4), EnvSpec(Alien-ramNoFrameskip-v0), EnvSpec(Alien-ramNoFrameskip-v4), EnvSpec(Amidar-v0), EnvSpec(Amidar-v4), EnvSpec(AmidarDeterministic-v0), EnvSpec(AmidarDeterministic-v4), EnvSpec(AmidarNoFrameskip-v0), EnvSpec(AmidarNoFrameskip-v4), EnvSpec(Amidar-ram-v0), EnvSpec(Amidar-ram-v4), EnvSpec(Amidar-ramDeterministic-v0), EnvSpec(Amidar-ramDeterministic-v4), EnvSpec(Amidar-ramNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v4), EnvSpec(Assault-v0), EnvSpec(Assault-v4), EnvSpec(AssaultDeterministic-v0), EnvSpec(AssaultDeterministic-v4), EnvSpec(AssaultNoFrameskip-v0), EnvSpec(AssaultNoFrameskip-v4), EnvSpec(Assault-ram-v0), EnvSpec(Assault-ram-v4), EnvSpec(Assault-ramDeterministic-v0), EnvSpec(Assault-ramDeterministic-v4), EnvSpec(Assault-ramNoFrameskip-v0), EnvSpec(Assault-ramNoFrameskip-v4), EnvSpec(Asterix-v0), EnvSpec(Asterix-v4), EnvSpec(AsterixDeterministic-v0), EnvSpec(AsterixDeterministic-v4), EnvSpec(AsterixNoFrameskip-v0), EnvSpec(AsterixNoFrameskip-v4), EnvSpec(Asterix-ram-v0), EnvSpec(Asterix-ram-v4), EnvSpec(Asterix-ramDeterministic-v0), EnvSpec(Asterix-ramDeterministic-v4), EnvSpec(Asterix-ramNoFrameskip-v0), EnvSpec(Asterix-ramNoFrameskip-v4), EnvSpec(Asteroids-v0), EnvSpec(Asteroids-v4), EnvSpec(AsteroidsDeterministic-v0), EnvSpec(AsteroidsDeterministic-v4), EnvSpec(AsteroidsNoFrameskip-v0), EnvSpec(AsteroidsNoFrameskip-v4), EnvSpec(Asteroids-ram-v0), EnvSpec(Asteroids-ram-v4), EnvSpec(Asteroids-ramDeterministic-v0), EnvSpec(Asteroids-ramDeterministic-v4), EnvSpec(Asteroids-ramNoFrameskip-v0), EnvSpec(Asteroids-ramNoFrameskip-v4), EnvSpec(Atlantis-v0), EnvSpec(Atlantis-v4), EnvSpec(AtlantisDeterministic-v0), EnvSpec(AtlantisDeterministic-v4), EnvSpec(AtlantisNoFrameskip-v0), EnvSpec(AtlantisNoFrameskip-v4), EnvSpec(Atlantis-ram-v0), EnvSpec(Atlantis-ram-v4), EnvSpec(Atlantis-ramDeterministic-v0), EnvSpec(Atlantis-ramDeterministic-v4), EnvSpec(Atlantis-ramNoFrameskip-v0), EnvSpec(Atlantis-ramNoFrameskip-v4), EnvSpec(BankHeist-v0), EnvSpec(BankHeist-v4), EnvSpec(BankHeistDeterministic-v0), EnvSpec(BankHeistDeterministic-v4), EnvSpec(BankHeistNoFrameskip-v0), EnvSpec(BankHeistNoFrameskip-v4), EnvSpec(BankHeist-ram-v0), EnvSpec(BankHeist-ram-v4), EnvSpec(BankHeist-ramDeterministic-v0), EnvSpec(BankHeist-ramDeterministic-v4), EnvSpec(BankHeist-ramNoFrameskip-v0), EnvSpec(BankHeist-ramNoFrameskip-v4), EnvSpec(BattleZone-v0), EnvSpec(BattleZone-v4), EnvSpec(BattleZoneDeterministic-v0), EnvSpec(BattleZoneDeterministic-v4), EnvSpec(BattleZoneNoFrameskip-v0), EnvSpec(BattleZoneNoFrameskip-v4), EnvSpec(BattleZone-ram-v0), EnvSpec(BattleZone-ram-v4), EnvSpec(BattleZone-ramDeterministic-v0), EnvSpec(BattleZone-ramDeterministic-v4), EnvSpec(BattleZone-ramNoFrameskip-v0), EnvSpec(BattleZone-ramNoFrameskip-v4), EnvSpec(BeamRider-v0), EnvSpec(BeamRider-v4), EnvSpec(BeamRiderDeterministic-v0), EnvSpec(BeamRiderDeterministic-v4), EnvSpec(BeamRiderNoFrameskip-v0), EnvSpec(BeamRiderNoFrameskip-v4), EnvSpec(BeamRider-ram-v0), EnvSpec(BeamRider-ram-v4), EnvSpec(BeamRider-ramDeterministic-v0), EnvSpec(BeamRider-ramDeterministic-v4), EnvSpec(BeamRider-ramNoFrameskip-v0), EnvSpec(BeamRider-ramNoFrameskip-v4), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-v4), EnvSpec(BerzerkDeterministic-v0), EnvSpec(BerzerkDeterministic-v4), EnvSpec(BerzerkNoFrameskip-v0), EnvSpec(BerzerkNoFrameskip-v4), EnvSpec(Berzerk-ram-v0), EnvSpec(Berzerk-ram-v4), EnvSpec(Berzerk-ramDeterministic-v0), EnvSpec(Berzerk-ramDeterministic-v4), EnvSpec(Berzerk-ramNoFrameskip-v0), EnvSpec(Berzerk-ramNoFrameskip-v4), EnvSpec(Bowling-v0), EnvSpec(Bowling-v4), EnvSpec(BowlingDeterministic-v0), EnvSpec(BowlingDeterministic-v4), EnvSpec(BowlingNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v4), EnvSpec(Bowling-ram-v0), EnvSpec(Bowling-ram-v4), EnvSpec(Bowling-ramDeterministic-v0), EnvSpec(Bowling-ramDeterministic-v4), EnvSpec(Bowling-ramNoFrameskip-v0), EnvSpec(Bowling-ramNoFrameskip-v4), EnvSpec(Boxing-v0), EnvSpec(Boxing-v4), EnvSpec(BoxingDeterministic-v0), EnvSpec(BoxingDeterministic-v4), EnvSpec(BoxingNoFrameskip-v0), EnvSpec(BoxingNoFrameskip-v4), EnvSpec(Boxing-ram-v0), EnvSpec(Boxing-ram-v4), EnvSpec(Boxing-ramDeterministic-v0), EnvSpec(Boxing-ramDeterministic-v4), EnvSpec(Boxing-ramNoFrameskip-v0), EnvSpec(Boxing-ramNoFrameskip-v4), EnvSpec(Breakout-v0), EnvSpec(Breakout-v4), EnvSpec(BreakoutDeterministic-v0), EnvSpec(BreakoutDeterministic-v4), EnvSpec(BreakoutNoFrameskip-v0), EnvSpec(BreakoutNoFrameskip-v4), EnvSpec(Breakout-ram-v0), EnvSpec(Breakout-ram-v4), EnvSpec(Breakout-ramDeterministic-v0), EnvSpec(Breakout-ramDeterministic-v4), EnvSpec(Breakout-ramNoFrameskip-v0), EnvSpec(Breakout-ramNoFrameskip-v4), EnvSpec(Carnival-v0), EnvSpec(Carnival-v4), EnvSpec(CarnivalDeterministic-v0), EnvSpec(CarnivalDeterministic-v4), EnvSpec(CarnivalNoFrameskip-v0), EnvSpec(CarnivalNoFrameskip-v4), EnvSpec(Carnival-ram-v0), EnvSpec(Carnival-ram-v4), EnvSpec(Carnival-ramDeterministic-v0), EnvSpec(Carnival-ramDeterministic-v4), EnvSpec(Carnival-ramNoFrameskip-v0), EnvSpec(Carnival-ramNoFrameskip-v4), EnvSpec(Centipede-v0), EnvSpec(Centipede-v4), EnvSpec(CentipedeDeterministic-v0), EnvSpec(CentipedeDeterministic-v4), EnvSpec(CentipedeNoFrameskip-v0), EnvSpec(CentipedeNoFrameskip-v4), EnvSpec(Centipede-ram-v0), EnvSpec(Centipede-ram-v4), EnvSpec(Centipede-ramDeterministic-v0), EnvSpec(Centipede-ramDeterministic-v4), EnvSpec(Centipede-ramNoFrameskip-v0), EnvSpec(Centipede-ramNoFrameskip-v4), EnvSpec(ChopperCommand-v0), EnvSpec(ChopperCommand-v4), EnvSpec(ChopperCommandDeterministic-v0), EnvSpec(ChopperCommandDeterministic-v4), EnvSpec(ChopperCommandNoFrameskip-v0), EnvSpec(ChopperCommandNoFrameskip-v4), EnvSpec(ChopperCommand-ram-v0), EnvSpec(ChopperCommand-ram-v4), EnvSpec(ChopperCommand-ramDeterministic-v0), EnvSpec(ChopperCommand-ramDeterministic-v4), EnvSpec(ChopperCommand-ramNoFrameskip-v0), EnvSpec(ChopperCommand-ramNoFrameskip-v4), EnvSpec(CrazyClimber-v0), EnvSpec(CrazyClimber-v4), EnvSpec(CrazyClimberDeterministic-v0), EnvSpec(CrazyClimberDeterministic-v4), EnvSpec(CrazyClimberNoFrameskip-v0), EnvSpec(CrazyClimberNoFrameskip-v4), EnvSpec(CrazyClimber-ram-v0), EnvSpec(CrazyClimber-ram-v4), EnvSpec(CrazyClimber-ramDeterministic-v0), EnvSpec(CrazyClimber-ramDeterministic-v4), EnvSpec(CrazyClimber-ramNoFrameskip-v0), EnvSpec(CrazyClimber-ramNoFrameskip-v4), EnvSpec(Defender-v0), EnvSpec(Defender-v4), EnvSpec(DefenderDeterministic-v0), EnvSpec(DefenderDeterministic-v4), EnvSpec(DefenderNoFrameskip-v0), EnvSpec(DefenderNoFrameskip-v4), EnvSpec(Defender-ram-v0), EnvSpec(Defender-ram-v4), EnvSpec(Defender-ramDeterministic-v0), EnvSpec(Defender-ramDeterministic-v4), EnvSpec(Defender-ramNoFrameskip-v0), EnvSpec(Defender-ramNoFrameskip-v4), EnvSpec(DemonAttack-v0), EnvSpec(DemonAttack-v4), EnvSpec(DemonAttackDeterministic-v0), EnvSpec(DemonAttackDeterministic-v4), EnvSpec(DemonAttackNoFrameskip-v0), EnvSpec(DemonAttackNoFrameskip-v4), EnvSpec(DemonAttack-ram-v0), EnvSpec(DemonAttack-ram-v4), EnvSpec(DemonAttack-ramDeterministic-v0), EnvSpec(DemonAttack-ramDeterministic-v4), EnvSpec(DemonAttack-ramNoFrameskip-v0), EnvSpec(DemonAttack-ramNoFrameskip-v4), EnvSpec(DoubleDunk-v0), EnvSpec(DoubleDunk-v4), EnvSpec(DoubleDunkDeterministic-v0), EnvSpec(DoubleDunkDeterministic-v4), EnvSpec(DoubleDunkNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v4), EnvSpec(DoubleDunk-ram-v0), EnvSpec(DoubleDunk-ram-v4), EnvSpec(DoubleDunk-ramDeterministic-v0), EnvSpec(DoubleDunk-ramDeterministic-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v0), EnvSpec(DoubleDunk-ramNoFrameskip-v4), EnvSpec(ElevatorAction-v0), EnvSpec(ElevatorAction-v4), EnvSpec(ElevatorActionDeterministic-v0), EnvSpec(ElevatorActionDeterministic-v4), EnvSpec(ElevatorActionNoFrameskip-v0), EnvSpec(ElevatorActionNoFrameskip-v4), EnvSpec(ElevatorAction-ram-v0), EnvSpec(ElevatorAction-ram-v4), EnvSpec(ElevatorAction-ramDeterministic-v0), EnvSpec(ElevatorAction-ramDeterministic-v4), EnvSpec(ElevatorAction-ramNoFrameskip-v0), EnvSpec(ElevatorAction-ramNoFrameskip-v4), EnvSpec(Enduro-v0), EnvSpec(Enduro-v4), EnvSpec(EnduroDeterministic-v0), EnvSpec(EnduroDeterministic-v4), EnvSpec(EnduroNoFrameskip-v0), EnvSpec(EnduroNoFrameskip-v4), EnvSpec(Enduro-ram-v0), EnvSpec(Enduro-ram-v4), EnvSpec(Enduro-ramDeterministic-v0), EnvSpec(Enduro-ramDeterministic-v4), EnvSpec(Enduro-ramNoFrameskip-v0), EnvSpec(Enduro-ramNoFrameskip-v4), EnvSpec(FishingDerby-v0), EnvSpec(FishingDerby-v4), EnvSpec(FishingDerbyDeterministic-v0), EnvSpec(FishingDerbyDeterministic-v4), EnvSpec(FishingDerbyNoFrameskip-v0), EnvSpec(FishingDerbyNoFrameskip-v4), EnvSpec(FishingDerby-ram-v0), EnvSpec(FishingDerby-ram-v4), EnvSpec(FishingDerby-ramDeterministic-v0), EnvSpec(FishingDerby-ramDeterministic-v4), EnvSpec(FishingDerby-ramNoFrameskip-v0), EnvSpec(FishingDerby-ramNoFrameskip-v4), EnvSpec(Freeway-v0), EnvSpec(Freeway-v4), EnvSpec(FreewayDeterministic-v0), EnvSpec(FreewayDeterministic-v4), EnvSpec(FreewayNoFrameskip-v0), EnvSpec(FreewayNoFrameskip-v4), EnvSpec(Freeway-ram-v0), EnvSpec(Freeway-ram-v4), EnvSpec(Freeway-ramDeterministic-v0), EnvSpec(Freeway-ramDeterministic-v4), EnvSpec(Freeway-ramNoFrameskip-v0), EnvSpec(Freeway-ramNoFrameskip-v4), EnvSpec(Frostbite-v0), EnvSpec(Frostbite-v4), EnvSpec(FrostbiteDeterministic-v0), EnvSpec(FrostbiteDeterministic-v4), EnvSpec(FrostbiteNoFrameskip-v0), EnvSpec(FrostbiteNoFrameskip-v4), EnvSpec(Frostbite-ram-v0), EnvSpec(Frostbite-ram-v4), EnvSpec(Frostbite-ramDeterministic-v0), EnvSpec(Frostbite-ramDeterministic-v4), EnvSpec(Frostbite-ramNoFrameskip-v0), EnvSpec(Frostbite-ramNoFrameskip-v4), EnvSpec(Gopher-v0), EnvSpec(Gopher-v4), EnvSpec(GopherDeterministic-v0), EnvSpec(GopherDeterministic-v4), EnvSpec(GopherNoFrameskip-v0), EnvSpec(GopherNoFrameskip-v4), EnvSpec(Gopher-ram-v0), EnvSpec(Gopher-ram-v4), EnvSpec(Gopher-ramDeterministic-v0), EnvSpec(Gopher-ramDeterministic-v4), EnvSpec(Gopher-ramNoFrameskip-v0), EnvSpec(Gopher-ramNoFrameskip-v4), EnvSpec(Gravitar-v0), EnvSpec(Gravitar-v4), EnvSpec(GravitarDeterministic-v0), EnvSpec(GravitarDeterministic-v4), EnvSpec(GravitarNoFrameskip-v0), EnvSpec(GravitarNoFrameskip-v4), EnvSpec(Gravitar-ram-v0), EnvSpec(Gravitar-ram-v4), EnvSpec(Gravitar-ramDeterministic-v0), EnvSpec(Gravitar-ramDeterministic-v4), EnvSpec(Gravitar-ramNoFrameskip-v0), EnvSpec(Gravitar-ramNoFrameskip-v4), EnvSpec(Hero-v0), EnvSpec(Hero-v4), EnvSpec(HeroDeterministic-v0), EnvSpec(HeroDeterministic-v4), EnvSpec(HeroNoFrameskip-v0), EnvSpec(HeroNoFrameskip-v4), EnvSpec(Hero-ram-v0), EnvSpec(Hero-ram-v4), EnvSpec(Hero-ramDeterministic-v0), EnvSpec(Hero-ramDeterministic-v4), EnvSpec(Hero-ramNoFrameskip-v0), EnvSpec(Hero-ramNoFrameskip-v4), EnvSpec(IceHockey-v0), EnvSpec(IceHockey-v4), EnvSpec(IceHockeyDeterministic-v0), EnvSpec(IceHockeyDeterministic-v4), EnvSpec(IceHockeyNoFrameskip-v0), EnvSpec(IceHockeyNoFrameskip-v4), EnvSpec(IceHockey-ram-v0), EnvSpec(IceHockey-ram-v4), EnvSpec(IceHockey-ramDeterministic-v0), EnvSpec(IceHockey-ramDeterministic-v4), EnvSpec(IceHockey-ramNoFrameskip-v0), EnvSpec(IceHockey-ramNoFrameskip-v4), EnvSpec(Jamesbond-v0), EnvSpec(Jamesbond-v4), EnvSpec(JamesbondDeterministic-v0), EnvSpec(JamesbondDeterministic-v4), EnvSpec(JamesbondNoFrameskip-v0), EnvSpec(JamesbondNoFrameskip-v4), EnvSpec(Jamesbond-ram-v0), EnvSpec(Jamesbond-ram-v4), EnvSpec(Jamesbond-ramDeterministic-v0), EnvSpec(Jamesbond-ramDeterministic-v4), EnvSpec(Jamesbond-ramNoFrameskip-v0), EnvSpec(Jamesbond-ramNoFrameskip-v4), EnvSpec(JourneyEscape-v0), EnvSpec(JourneyEscape-v4), EnvSpec(JourneyEscapeDeterministic-v0), EnvSpec(JourneyEscapeDeterministic-v4), EnvSpec(JourneyEscapeNoFrameskip-v0), EnvSpec(JourneyEscapeNoFrameskip-v4), EnvSpec(JourneyEscape-ram-v0), EnvSpec(JourneyEscape-ram-v4), EnvSpec(JourneyEscape-ramDeterministic-v0), EnvSpec(JourneyEscape-ramDeterministic-v4), EnvSpec(JourneyEscape-ramNoFrameskip-v0), EnvSpec(JourneyEscape-ramNoFrameskip-v4), EnvSpec(Kangaroo-v0), EnvSpec(Kangaroo-v4), EnvSpec(KangarooDeterministic-v0), EnvSpec(KangarooDeterministic-v4), EnvSpec(KangarooNoFrameskip-v0), EnvSpec(KangarooNoFrameskip-v4), EnvSpec(Kangaroo-ram-v0), EnvSpec(Kangaroo-ram-v4), EnvSpec(Kangaroo-ramDeterministic-v0), EnvSpec(Kangaroo-ramDeterministic-v4), EnvSpec(Kangaroo-ramNoFrameskip-v0), EnvSpec(Kangaroo-ramNoFrameskip-v4), EnvSpec(Krull-v0), EnvSpec(Krull-v4), EnvSpec(KrullDeterministic-v0), EnvSpec(KrullDeterministic-v4), EnvSpec(KrullNoFrameskip-v0), EnvSpec(KrullNoFrameskip-v4), EnvSpec(Krull-ram-v0), EnvSpec(Krull-ram-v4), EnvSpec(Krull-ramDeterministic-v0), EnvSpec(Krull-ramDeterministic-v4), EnvSpec(Krull-ramNoFrameskip-v0), EnvSpec(Krull-ramNoFrameskip-v4), EnvSpec(KungFuMaster-v0), EnvSpec(KungFuMaster-v4), EnvSpec(KungFuMasterDeterministic-v0), EnvSpec(KungFuMasterDeterministic-v4), EnvSpec(KungFuMasterNoFrameskip-v0), EnvSpec(KungFuMasterNoFrameskip-v4), EnvSpec(KungFuMaster-ram-v0), EnvSpec(KungFuMaster-ram-v4), EnvSpec(KungFuMaster-ramDeterministic-v0), EnvSpec(KungFuMaster-ramDeterministic-v4), EnvSpec(KungFuMaster-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v4), EnvSpec(MontezumaRevenge-v0), EnvSpec(MontezumaRevenge-v4), EnvSpec(MontezumaRevengeDeterministic-v0), EnvSpec(MontezumaRevengeDeterministic-v4), EnvSpec(MontezumaRevengeNoFrameskip-v0), EnvSpec(MontezumaRevengeNoFrameskip-v4), EnvSpec(MontezumaRevenge-ram-v0), EnvSpec(MontezumaRevenge-ram-v4), EnvSpec(MontezumaRevenge-ramDeterministic-v0), EnvSpec(MontezumaRevenge-ramDeterministic-v4), EnvSpec(MontezumaRevenge-ramNoFrameskip-v0), EnvSpec(MontezumaRevenge-ramNoFrameskip-v4), EnvSpec(MsPacman-v0), EnvSpec(MsPacman-v4), EnvSpec(MsPacmanDeterministic-v0), EnvSpec(MsPacmanDeterministic-v4), EnvSpec(MsPacmanNoFrameskip-v0), EnvSpec(MsPacmanNoFrameskip-v4), EnvSpec(MsPacman-ram-v0), EnvSpec(MsPacman-ram-v4), EnvSpec(MsPacman-ramDeterministic-v0), EnvSpec(MsPacman-ramDeterministic-v4), EnvSpec(MsPacman-ramNoFrameskip-v0), EnvSpec(MsPacman-ramNoFrameskip-v4), EnvSpec(NameThisGame-v0), EnvSpec(NameThisGame-v4), EnvSpec(NameThisGameDeterministic-v0), EnvSpec(NameThisGameDeterministic-v4), EnvSpec(NameThisGameNoFrameskip-v0), EnvSpec(NameThisGameNoFrameskip-v4), EnvSpec(NameThisGame-ram-v0), EnvSpec(NameThisGame-ram-v4), EnvSpec(NameThisGame-ramDeterministic-v0), EnvSpec(NameThisGame-ramDeterministic-v4), EnvSpec(NameThisGame-ramNoFrameskip-v0), EnvSpec(NameThisGame-ramNoFrameskip-v4), EnvSpec(Phoenix-v0), EnvSpec(Phoenix-v4), EnvSpec(PhoenixDeterministic-v0), EnvSpec(PhoenixDeterministic-v4), EnvSpec(PhoenixNoFrameskip-v0), EnvSpec(PhoenixNoFrameskip-v4), EnvSpec(Phoenix-ram-v0), EnvSpec(Phoenix-ram-v4), EnvSpec(Phoenix-ramDeterministic-v0), EnvSpec(Phoenix-ramDeterministic-v4), EnvSpec(Phoenix-ramNoFrameskip-v0), EnvSpec(Phoenix-ramNoFrameskip-v4), EnvSpec(Pitfall-v0), EnvSpec(Pitfall-v4), EnvSpec(PitfallDeterministic-v0), EnvSpec(PitfallDeterministic-v4), EnvSpec(PitfallNoFrameskip-v0), EnvSpec(PitfallNoFrameskip-v4), EnvSpec(Pitfall-ram-v0), EnvSpec(Pitfall-ram-v4), EnvSpec(Pitfall-ramDeterministic-v0), EnvSpec(Pitfall-ramDeterministic-v4), EnvSpec(Pitfall-ramNoFrameskip-v0), EnvSpec(Pitfall-ramNoFrameskip-v4), EnvSpec(Pong-v0), EnvSpec(Pong-v4), EnvSpec(PongDeterministic-v0), EnvSpec(PongDeterministic-v4), EnvSpec(PongNoFrameskip-v0), EnvSpec(PongNoFrameskip-v4), EnvSpec(Pong-ram-v0), EnvSpec(Pong-ram-v4), EnvSpec(Pong-ramDeterministic-v0), EnvSpec(Pong-ramDeterministic-v4), EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(Pong-ramNoFrameskip-v4), EnvSpec(Pooyan-v0), EnvSpec(Pooyan-v4), EnvSpec(PooyanDeterministic-v0), EnvSpec(PooyanDeterministic-v4), EnvSpec(PooyanNoFrameskip-v0), EnvSpec(PooyanNoFrameskip-v4), EnvSpec(Pooyan-ram-v0), EnvSpec(Pooyan-ram-v4), EnvSpec(Pooyan-ramDeterministic-v0), EnvSpec(Pooyan-ramDeterministic-v4), EnvSpec(Pooyan-ramNoFrameskip-v0), EnvSpec(Pooyan-ramNoFrameskip-v4), EnvSpec(PrivateEye-v0), EnvSpec(PrivateEye-v4), EnvSpec(PrivateEyeDeterministic-v0), EnvSpec(PrivateEyeDeterministic-v4), EnvSpec(PrivateEyeNoFrameskip-v0), EnvSpec(PrivateEyeNoFrameskip-v4), EnvSpec(PrivateEye-ram-v0), EnvSpec(PrivateEye-ram-v4), EnvSpec(PrivateEye-ramDeterministic-v0), EnvSpec(PrivateEye-ramDeterministic-v4), EnvSpec(PrivateEye-ramNoFrameskip-v0), EnvSpec(PrivateEye-ramNoFrameskip-v4), EnvSpec(Qbert-v0), EnvSpec(Qbert-v4), EnvSpec(QbertDeterministic-v0), EnvSpec(QbertDeterministic-v4), EnvSpec(QbertNoFrameskip-v0), EnvSpec(QbertNoFrameskip-v4), EnvSpec(Qbert-ram-v0), EnvSpec(Qbert-ram-v4), EnvSpec(Qbert-ramDeterministic-v0), EnvSpec(Qbert-ramDeterministic-v4), EnvSpec(Qbert-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v4), EnvSpec(Riverraid-v0), EnvSpec(Riverraid-v4), EnvSpec(RiverraidDeterministic-v0), EnvSpec(RiverraidDeterministic-v4), EnvSpec(RiverraidNoFrameskip-v0), EnvSpec(RiverraidNoFrameskip-v4), EnvSpec(Riverraid-ram-v0), EnvSpec(Riverraid-ram-v4), EnvSpec(Riverraid-ramDeterministic-v0), EnvSpec(Riverraid-ramDeterministic-v4), EnvSpec(Riverraid-ramNoFrameskip-v0), EnvSpec(Riverraid-ramNoFrameskip-v4), EnvSpec(RoadRunner-v0), EnvSpec(RoadRunner-v4), EnvSpec(RoadRunnerDeterministic-v0), EnvSpec(RoadRunnerDeterministic-v4), EnvSpec(RoadRunnerNoFrameskip-v0), EnvSpec(RoadRunnerNoFrameskip-v4), EnvSpec(RoadRunner-ram-v0), EnvSpec(RoadRunner-ram-v4), EnvSpec(RoadRunner-ramDeterministic-v0), EnvSpec(RoadRunner-ramDeterministic-v4), EnvSpec(RoadRunner-ramNoFrameskip-v0), EnvSpec(RoadRunner-ramNoFrameskip-v4), EnvSpec(Robotank-v0), EnvSpec(Robotank-v4), EnvSpec(RobotankDeterministic-v0), EnvSpec(RobotankDeterministic-v4), EnvSpec(RobotankNoFrameskip-v0), EnvSpec(RobotankNoFrameskip-v4), EnvSpec(Robotank-ram-v0), EnvSpec(Robotank-ram-v4), EnvSpec(Robotank-ramDeterministic-v0), EnvSpec(Robotank-ramDeterministic-v4), EnvSpec(Robotank-ramNoFrameskip-v0), EnvSpec(Robotank-ramNoFrameskip-v4), EnvSpec(Seaquest-v0), EnvSpec(Seaquest-v4), EnvSpec(SeaquestDeterministic-v0), EnvSpec(SeaquestDeterministic-v4), EnvSpec(SeaquestNoFrameskip-v0), EnvSpec(SeaquestNoFrameskip-v4), EnvSpec(Seaquest-ram-v0), EnvSpec(Seaquest-ram-v4), EnvSpec(Seaquest-ramDeterministic-v0), EnvSpec(Seaquest-ramDeterministic-v4), EnvSpec(Seaquest-ramNoFrameskip-v0), EnvSpec(Seaquest-ramNoFrameskip-v4), EnvSpec(Skiing-v0), EnvSpec(Skiing-v4), EnvSpec(SkiingDeterministic-v0), EnvSpec(SkiingDeterministic-v4), EnvSpec(SkiingNoFrameskip-v0), EnvSpec(SkiingNoFrameskip-v4), EnvSpec(Skiing-ram-v0), EnvSpec(Skiing-ram-v4), EnvSpec(Skiing-ramDeterministic-v0), EnvSpec(Skiing-ramDeterministic-v4), EnvSpec(Skiing-ramNoFrameskip-v0), EnvSpec(Skiing-ramNoFrameskip-v4), EnvSpec(Solaris-v0), EnvSpec(Solaris-v4), EnvSpec(SolarisDeterministic-v0), EnvSpec(SolarisDeterministic-v4), EnvSpec(SolarisNoFrameskip-v0), EnvSpec(SolarisNoFrameskip-v4), EnvSpec(Solaris-ram-v0), EnvSpec(Solaris-ram-v4), EnvSpec(Solaris-ramDeterministic-v0), EnvSpec(Solaris-ramDeterministic-v4), EnvSpec(Solaris-ramNoFrameskip-v0), EnvSpec(Solaris-ramNoFrameskip-v4), EnvSpec(SpaceInvaders-v0), EnvSpec(SpaceInvaders-v4), EnvSpec(SpaceInvadersDeterministic-v0), EnvSpec(SpaceInvadersDeterministic-v4), EnvSpec(SpaceInvadersNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v4), EnvSpec(SpaceInvaders-ram-v0), EnvSpec(SpaceInvaders-ram-v4), EnvSpec(SpaceInvaders-ramDeterministic-v0), EnvSpec(SpaceInvaders-ramDeterministic-v4), EnvSpec(SpaceInvaders-ramNoFrameskip-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v4), EnvSpec(StarGunner-v0), EnvSpec(StarGunner-v4), EnvSpec(StarGunnerDeterministic-v0), EnvSpec(StarGunnerDeterministic-v4), EnvSpec(StarGunnerNoFrameskip-v0), EnvSpec(StarGunnerNoFrameskip-v4), EnvSpec(StarGunner-ram-v0), EnvSpec(StarGunner-ram-v4), EnvSpec(StarGunner-ramDeterministic-v0), EnvSpec(StarGunner-ramDeterministic-v4), EnvSpec(StarGunner-ramNoFrameskip-v0), EnvSpec(StarGunner-ramNoFrameskip-v4), EnvSpec(Tennis-v0), EnvSpec(Tennis-v4), EnvSpec(TennisDeterministic-v0), EnvSpec(TennisDeterministic-v4), EnvSpec(TennisNoFrameskip-v0), EnvSpec(TennisNoFrameskip-v4), EnvSpec(Tennis-ram-v0), EnvSpec(Tennis-ram-v4), EnvSpec(Tennis-ramDeterministic-v0), EnvSpec(Tennis-ramDeterministic-v4), EnvSpec(Tennis-ramNoFrameskip-v0), EnvSpec(Tennis-ramNoFrameskip-v4), EnvSpec(TimePilot-v0), EnvSpec(TimePilot-v4), EnvSpec(TimePilotDeterministic-v0), EnvSpec(TimePilotDeterministic-v4), EnvSpec(TimePilotNoFrameskip-v0), EnvSpec(TimePilotNoFrameskip-v4), EnvSpec(TimePilot-ram-v0), EnvSpec(TimePilot-ram-v4), EnvSpec(TimePilot-ramDeterministic-v0), EnvSpec(TimePilot-ramDeterministic-v4), EnvSpec(TimePilot-ramNoFrameskip-v0), EnvSpec(TimePilot-ramNoFrameskip-v4), EnvSpec(Tutankham-v0), EnvSpec(Tutankham-v4), EnvSpec(TutankhamDeterministic-v0), EnvSpec(TutankhamDeterministic-v4), EnvSpec(TutankhamNoFrameskip-v0), EnvSpec(TutankhamNoFrameskip-v4), EnvSpec(Tutankham-ram-v0), EnvSpec(Tutankham-ram-v4), EnvSpec(Tutankham-ramDeterministic-v0), EnvSpec(Tutankham-ramDeterministic-v4), EnvSpec(Tutankham-ramNoFrameskip-v0), EnvSpec(Tutankham-ramNoFrameskip-v4), EnvSpec(UpNDown-v0), EnvSpec(UpNDown-v4), EnvSpec(UpNDownDeterministic-v0), EnvSpec(UpNDownDeterministic-v4), EnvSpec(UpNDownNoFrameskip-v0), EnvSpec(UpNDownNoFrameskip-v4), EnvSpec(UpNDown-ram-v0), EnvSpec(UpNDown-ram-v4), EnvSpec(UpNDown-ramDeterministic-v0), EnvSpec(UpNDown-ramDeterministic-v4), EnvSpec(UpNDown-ramNoFrameskip-v0), EnvSpec(UpNDown-ramNoFrameskip-v4), EnvSpec(Venture-v0), EnvSpec(Venture-v4), EnvSpec(VentureDeterministic-v0), EnvSpec(VentureDeterministic-v4), EnvSpec(VentureNoFrameskip-v0), EnvSpec(VentureNoFrameskip-v4), EnvSpec(Venture-ram-v0), EnvSpec(Venture-ram-v4), EnvSpec(Venture-ramDeterministic-v0), EnvSpec(Venture-ramDeterministic-v4), EnvSpec(Venture-ramNoFrameskip-v0), EnvSpec(Venture-ramNoFrameskip-v4), EnvSpec(VideoPinball-v0), EnvSpec(VideoPinball-v4), EnvSpec(VideoPinballDeterministic-v0), EnvSpec(VideoPinballDeterministic-v4), EnvSpec(VideoPinballNoFrameskip-v0), EnvSpec(VideoPinballNoFrameskip-v4), EnvSpec(VideoPinball-ram-v0), EnvSpec(VideoPinball-ram-v4), EnvSpec(VideoPinball-ramDeterministic-v0), EnvSpec(VideoPinball-ramDeterministic-v4), EnvSpec(VideoPinball-ramNoFrameskip-v0), EnvSpec(VideoPinball-ramNoFrameskip-v4), EnvSpec(WizardOfWor-v0), EnvSpec(WizardOfWor-v4), EnvSpec(WizardOfWorDeterministic-v0), EnvSpec(WizardOfWorDeterministic-v4), EnvSpec(WizardOfWorNoFrameskip-v0), EnvSpec(WizardOfWorNoFrameskip-v4), EnvSpec(WizardOfWor-ram-v0), EnvSpec(WizardOfWor-ram-v4), EnvSpec(WizardOfWor-ramDeterministic-v0), EnvSpec(WizardOfWor-ramDeterministic-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v0), EnvSpec(WizardOfWor-ramNoFrameskip-v4), EnvSpec(YarsRevenge-v0), EnvSpec(YarsRevenge-v4), EnvSpec(YarsRevengeDeterministic-v0), EnvSpec(YarsRevengeDeterministic-v4), EnvSpec(YarsRevengeNoFrameskip-v0), EnvSpec(YarsRevengeNoFrameskip-v4), EnvSpec(YarsRevenge-ram-v0), EnvSpec(YarsRevenge-ram-v4), EnvSpec(YarsRevenge-ramDeterministic-v0), EnvSpec(YarsRevenge-ramDeterministic-v4), EnvSpec(YarsRevenge-ramNoFrameskip-v0), EnvSpec(YarsRevenge-ramNoFrameskip-v4), EnvSpec(Zaxxon-v0), EnvSpec(Zaxxon-v4), EnvSpec(ZaxxonDeterministic-v0), EnvSpec(ZaxxonDeterministic-v4), EnvSpec(ZaxxonNoFrameskip-v0), EnvSpec(ZaxxonNoFrameskip-v4), EnvSpec(Zaxxon-ram-v0), EnvSpec(Zaxxon-ram-v4), EnvSpec(Zaxxon-ramDeterministic-v0), EnvSpec(Zaxxon-ramDeterministic-v4), EnvSpec(Zaxxon-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v4), EnvSpec(CubeCrash-v0), EnvSpec(CubeCrashSparse-v0), EnvSpec(CubeCrashScreenBecomesBlack-v0), EnvSpec(MemorizeDigits-v0)])\n"
     ]
    }
   ],
   "source": [
    "print(gym.envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HP():\n",
    "    # Hyperparameters\n",
    "    def __init__(self,\n",
    "                 nb_steps=1000,\n",
    "                 episode_length=2000,\n",
    "                 learning_rate=0.02,\n",
    "                 num_deltas=16,\n",
    "                 num_best_deltas=16,\n",
    "                 noise=0.03,\n",
    "                 seed=1,\n",
    "                 env_name='BipedalWalker-v2',\n",
    "                 record_every=50):\n",
    "\n",
    "        self.nb_steps = nb_steps\n",
    "        self.episode_length = episode_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_deltas = num_deltas\n",
    "        self.num_best_deltas = num_best_deltas\n",
    "        assert self.num_best_deltas <= self.num_deltas\n",
    "        self.noise = noise\n",
    "        self.seed = seed\n",
    "        self.env_name = env_name\n",
    "        self.record_every = record_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    # Normalizes the inputs\n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.0\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self, input_size, output_size, hp):\n",
    "      #this creates a zero matrix of rows,column\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "        self.hp = hp\n",
    "\n",
    "    def evaluate(self, input, delta = None, direction = None):\n",
    "        if direction is None:\n",
    "          #.dot is numpy function for dot product\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"+\":\n",
    "            return (self.theta + self.hp.noise * delta).dot(input)\n",
    "        elif direction == \"-\":\n",
    "            return (self.theta - self.hp.noise * delta).dot(input)\n",
    "\n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(self.hp.num_deltas)]\n",
    "#This code above here is super important \n",
    "#This is how the weights are updated according to which configuration of weights led to the biggest reward\n",
    "    def update(self, rollouts, sigma_rewards):\n",
    "        # sigma_rewards is the standard deviation of the rewards\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, delta in rollouts:\n",
    "            step += (r_pos - r_neg) * delta\n",
    "        self.theta += self.hp.learning_rate / (self.hp.num_best_deltas * sigma_rewards) * step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARSTrainer():\n",
    "    def __init__(self,\n",
    "                 hp=None,\n",
    "                 input_size=None,\n",
    "                 output_size=None,\n",
    "                 normalizer=None,\n",
    "                 policy=None,\n",
    "                 monitor_dir=None):\n",
    "\n",
    "        self.hp = hp or HP()\n",
    "        np.random.seed(self.hp.seed)\n",
    "        self.env = gym.make(self.hp.env_name)\n",
    "        if monitor_dir is not None:\n",
    "            should_record = lambda i: self.record_video\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)\n",
    "        self.hp.episode_length = self.hp.episode_length\n",
    "        self.input_size = input_size or self.env.observation_space.shape[0]\n",
    "        self.output_size = output_size or self.env.action_space.shape[0]\n",
    "        self.normalizer = normalizer or Normalizer(self.input_size)\n",
    "        self.policy = policy or Policy(self.input_size, self.output_size, self.hp)\n",
    "        self.record_video = False\n",
    "\n",
    "    # Explore the policy on one specific direction and over one episode\n",
    "    def explore(self, direction=None, delta=None):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.0\n",
    "        sum_rewards = 0.0\n",
    "        while not done and num_plays < self.hp.episode_length:\n",
    "            self.normalizer.observe(state)\n",
    "            state = self.normalizer.normalize(state)\n",
    "            action = self.policy.evaluate(state, delta, direction)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            reward = max(min(reward, 1), -1)\n",
    "            sum_rewards += reward\n",
    "            num_plays += 1\n",
    "        return sum_rewards\n",
    "\n",
    "    def train(self):\n",
    "        for step in range(self.hp.nb_steps):\n",
    "            # initialize the random noise deltas and the positive/negative rewards\n",
    "            deltas = self.policy.sample_deltas()\n",
    "            positive_rewards = [0] * self.hp.num_deltas\n",
    "            negative_rewards = [0] * self.hp.num_deltas\n",
    "\n",
    "            # play an episode each with positive deltas and negative deltas, collect rewards\n",
    "            for k in range(self.hp.num_deltas):\n",
    "                positive_rewards[k] = self.explore(direction=\"+\", delta=deltas[k])\n",
    "                negative_rewards[k] = self.explore(direction=\"-\", delta=deltas[k])\n",
    "                \n",
    "            # Compute the standard deviation of all rewards\n",
    "            sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
    "\n",
    "            # Sort the rollouts by the max(r_pos, r_neg) and select the deltas with best rewards\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:self.hp.num_best_deltas]\n",
    "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            # Update the policy\n",
    "            self.policy.update(rollouts, sigma_rewards)\n",
    "\n",
    "            # Only record video during evaluation, every n steps\n",
    "            if step % self.hp.record_every == 0:\n",
    "                self.record_video = True\n",
    "            # Play an episode with the new weights and print the score\n",
    "            reward_evaluation = self.explore()\n",
    "            print('Step: ', step, 'Reward: ', reward_evaluation)\n",
    "            self.record_video = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 Reward:  7.167685932216035\n",
      "Step:  1 Reward:  5.181421499445483\n",
      "Step:  2 Reward:  5.204582957056703\n",
      "Step:  3 Reward:  5.146906314415738\n",
      "Step:  4 Reward:  6.130994492327291\n",
      "Time elapsed: 0:00:28.351400\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "ENV_NAME = 'BipedalWalker-v3'\n",
    "\n",
    "videos_dir = mkdir('.', 'videos')\n",
    "monitor_dir = mkdir(videos_dir, ENV_NAME)\n",
    "\n",
    "hp = HP(env_name=ENV_NAME, nb_steps=5)\n",
    "trainer = ARSTrainer(hp=hp, monitor_dir=monitor_dir)\n",
    "trainer.train()\n",
    "end = datetime.datetime.now()\n",
    "print(\"Time elapsed:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 Reward:  7.201754848995174\n",
      "Step:  1 Reward:  5.910035305126789\n",
      "Step:  2 Reward:  -16.325964244665336\n",
      "Step:  3 Reward:  6.125203420809023\n",
      "Step:  4 Reward:  3.3585454726996655\n",
      "Step:  5 Reward:  2.7376328079717105\n",
      "Step:  6 Reward:  2.9403991365301834\n",
      "Step:  7 Reward:  2.251316898508081\n",
      "Step:  8 Reward:  3.1733946027911557\n",
      "Step:  9 Reward:  2.8667793908242096\n",
      "Step:  10 Reward:  3.869940680240518\n",
      "Step:  11 Reward:  4.009611958498104\n",
      "Step:  12 Reward:  4.07066822153234\n",
      "Step:  13 Reward:  3.6079994315117068\n",
      "Step:  14 Reward:  3.522110822229344\n",
      "Step:  15 Reward:  6.324855491124692\n",
      "Step:  16 Reward:  5.620884462282735\n",
      "Step:  17 Reward:  4.568540984787917\n",
      "Step:  18 Reward:  4.268912809356342\n",
      "Step:  19 Reward:  4.689601616519572\n",
      "Step:  20 Reward:  5.06420993279569\n",
      "Step:  21 Reward:  4.296691491191144\n",
      "Step:  22 Reward:  3.819935711374087\n",
      "Step:  23 Reward:  3.7273745162465337\n",
      "Step:  24 Reward:  4.071615534971969\n",
      "Step:  25 Reward:  3.4852839784281517\n",
      "Step:  26 Reward:  4.532317356613178\n",
      "Step:  27 Reward:  5.332020376700524\n",
      "Step:  28 Reward:  -5.164218407322484\n",
      "Step:  29 Reward:  2.7173704920275705\n",
      "Step:  30 Reward:  3.9660698449673264\n",
      "Step:  31 Reward:  3.819903846574065\n",
      "Step:  32 Reward:  4.079570282799206\n",
      "Step:  33 Reward:  4.098904979106965\n",
      "Step:  34 Reward:  4.132609766683576\n",
      "Step:  35 Reward:  4.334228975871789\n",
      "Step:  36 Reward:  4.3041762584323555\n",
      "Step:  37 Reward:  4.569254793468168\n",
      "Step:  38 Reward:  4.793187740298545\n",
      "Step:  39 Reward:  3.991942843698725\n",
      "Step:  40 Reward:  5.156465095381589\n",
      "Step:  41 Reward:  4.303718925042791\n",
      "Step:  42 Reward:  5.1218650265087025\n",
      "Step:  43 Reward:  4.483875210259766\n",
      "Step:  44 Reward:  4.610639206063661\n",
      "Step:  45 Reward:  5.146845932084862\n",
      "Step:  46 Reward:  5.2807442400522335\n",
      "Step:  47 Reward:  4.527099472365265\n",
      "Step:  48 Reward:  5.006879644885913\n",
      "Step:  49 Reward:  4.046096997279206\n",
      "Step:  50 Reward:  3.8837464248041504\n",
      "Step:  51 Reward:  0.39941693114259613\n",
      "Step:  52 Reward:  3.4384065538535875\n",
      "Step:  53 Reward:  4.343927244336811\n",
      "Step:  54 Reward:  4.317756669441688\n",
      "Step:  55 Reward:  5.0105947734187435\n",
      "Step:  56 Reward:  4.776572766003689\n",
      "Step:  57 Reward:  3.570573935952175\n",
      "Step:  58 Reward:  3.8318396211834997\n",
      "Step:  59 Reward:  5.11268272473103\n",
      "Step:  60 Reward:  4.7190250873703095\n",
      "Step:  61 Reward:  4.670237685686447\n",
      "Step:  62 Reward:  5.294826599517827\n",
      "Step:  63 Reward:  4.639656685408121\n",
      "Step:  64 Reward:  4.300606391384523\n",
      "Step:  65 Reward:  5.545741738370243\n",
      "Step:  66 Reward:  4.967846622047138\n",
      "Step:  67 Reward:  5.484978825628214\n",
      "Step:  68 Reward:  4.762442818149275\n",
      "Step:  69 Reward:  4.683855278895476\n",
      "Step:  70 Reward:  5.317465440299817\n",
      "Step:  71 Reward:  4.893212917824129\n",
      "Step:  72 Reward:  5.016034132853312\n",
      "Step:  73 Reward:  5.1763090535738\n",
      "Step:  74 Reward:  5.228386781172389\n",
      "Step:  75 Reward:  4.944808979689977\n",
      "Step:  76 Reward:  5.7230890677609425\n",
      "Step:  77 Reward:  5.808720142458278\n",
      "Step:  78 Reward:  5.77594798592432\n",
      "Step:  79 Reward:  5.90295008147939\n",
      "Step:  80 Reward:  5.527592089210439\n",
      "Step:  81 Reward:  5.618733879261005\n",
      "Step:  82 Reward:  5.6345635644727095\n",
      "Step:  83 Reward:  5.216562901803674\n",
      "Step:  84 Reward:  4.834279042245785\n",
      "Step:  85 Reward:  4.7428445381052615\n",
      "Step:  86 Reward:  5.813897167461776\n",
      "Step:  87 Reward:  5.374069903208536\n",
      "Step:  88 Reward:  5.1721658351708975\n",
      "Step:  89 Reward:  5.887578536918604\n",
      "Step:  90 Reward:  5.763143060522876\n",
      "Step:  91 Reward:  5.821675819004346\n",
      "Step:  92 Reward:  5.712397521349863\n",
      "Step:  93 Reward:  5.814786942294835\n",
      "Step:  94 Reward:  5.720318853477761\n",
      "Step:  95 Reward:  6.095181073766357\n",
      "Step:  96 Reward:  6.209007116150057\n",
      "Step:  97 Reward:  6.068181193489911\n",
      "Step:  98 Reward:  6.918483334705117\n",
      "Step:  99 Reward:  5.682532558387096\n",
      "Step:  100 Reward:  6.585124875942434\n",
      "Step:  101 Reward:  6.02110572990565\n",
      "Step:  102 Reward:  5.55736605094426\n",
      "Step:  103 Reward:  6.488996524967062\n",
      "Step:  104 Reward:  6.854504227676463\n",
      "Step:  105 Reward:  6.270559804809958\n",
      "Step:  106 Reward:  6.269101209381587\n",
      "Step:  107 Reward:  6.3377514418639995\n",
      "Step:  108 Reward:  6.181766899024041\n",
      "Step:  109 Reward:  5.227657887576995\n",
      "Step:  110 Reward:  6.889210139027537\n",
      "Step:  111 Reward:  5.42975322826268\n",
      "Step:  112 Reward:  6.861099138566986\n",
      "Step:  113 Reward:  6.2385737594479584\n",
      "Step:  114 Reward:  6.710323093313339\n",
      "Step:  115 Reward:  6.725357685050897\n",
      "Step:  116 Reward:  6.766820822091445\n",
      "Step:  117 Reward:  6.9248053983091475\n",
      "Step:  118 Reward:  6.835202743077072\n",
      "Step:  119 Reward:  6.893143166283329\n",
      "Step:  120 Reward:  6.120080978494566\n",
      "Step:  121 Reward:  6.543483245325637\n",
      "Step:  122 Reward:  6.416493301255689\n",
      "Step:  123 Reward:  5.55488588967691\n",
      "Step:  124 Reward:  6.187258267860491\n",
      "Step:  125 Reward:  6.7678033093352585\n",
      "Step:  126 Reward:  -26.015464517659897\n",
      "Step:  127 Reward:  7.371545289032229\n",
      "Step:  128 Reward:  -34.50829639049599\n",
      "Step:  129 Reward:  8.307219063255593\n",
      "Step:  130 Reward:  7.841141988841267\n",
      "Step:  131 Reward:  7.767748221813827\n",
      "Step:  132 Reward:  6.110041689771599\n",
      "Step:  133 Reward:  6.478897261445043\n",
      "Step:  134 Reward:  6.095747135019729\n",
      "Step:  135 Reward:  6.817298865727808\n",
      "Step:  136 Reward:  7.250683995056189\n",
      "Step:  137 Reward:  7.254347051207997\n",
      "Step:  138 Reward:  6.546096197773505\n",
      "Step:  139 Reward:  7.349214317041209\n",
      "Step:  140 Reward:  7.603589349053777\n",
      "Step:  141 Reward:  6.8755579093784025\n",
      "Step:  142 Reward:  7.438243342276381\n",
      "Step:  143 Reward:  7.523839570656984\n",
      "Step:  144 Reward:  6.932021062453535\n",
      "Step:  145 Reward:  7.533937105465352\n",
      "Step:  146 Reward:  8.149427163661548\n",
      "Step:  147 Reward:  9.219544118906821\n",
      "Step:  148 Reward:  7.5947183560369815\n",
      "Step:  149 Reward:  8.902130107693772\n",
      "Step:  150 Reward:  9.522880045904193\n",
      "Step:  151 Reward:  8.00555356662781\n",
      "Step:  152 Reward:  7.848025648423466\n",
      "Step:  153 Reward:  8.328584588969928\n",
      "Step:  154 Reward:  9.327004420537891\n",
      "Step:  155 Reward:  8.993520678067773\n",
      "Step:  156 Reward:  7.565801444271537\n",
      "Step:  157 Reward:  8.083160799273122\n",
      "Step:  158 Reward:  8.421785130231443\n",
      "Step:  159 Reward:  7.43480361990202\n",
      "Step:  160 Reward:  7.293120690185116\n",
      "Step:  161 Reward:  7.102702325748076\n",
      "Step:  162 Reward:  8.327809089649787\n",
      "Step:  163 Reward:  8.005750729778553\n",
      "Step:  164 Reward:  8.785550924899255\n",
      "Step:  165 Reward:  8.827965704959437\n",
      "Step:  166 Reward:  8.250403467697492\n",
      "Step:  167 Reward:  8.056148194629193\n",
      "Step:  168 Reward:  7.492627521664737\n",
      "Step:  169 Reward:  7.788361240431186\n",
      "Step:  170 Reward:  7.82468476767208\n",
      "Step:  171 Reward:  8.25535236475746\n",
      "Step:  172 Reward:  7.410653381192224\n",
      "Step:  173 Reward:  7.166967739531161\n",
      "Step:  174 Reward:  7.937517643672278\n",
      "Step:  175 Reward:  7.8090879481034285\n",
      "Step:  176 Reward:  8.35291761902252\n",
      "Step:  177 Reward:  7.431161152518932\n",
      "Step:  178 Reward:  8.354822363157247\n",
      "Step:  179 Reward:  8.36365173329716\n",
      "Step:  180 Reward:  9.071882079710095\n",
      "Step:  181 Reward:  8.910132442627217\n",
      "Step:  182 Reward:  9.969671693604557\n",
      "Step:  183 Reward:  9.357019287715012\n",
      "Step:  184 Reward:  9.095351348505826\n",
      "Step:  185 Reward:  8.181716497597602\n",
      "Step:  186 Reward:  8.711836263194908\n",
      "Step:  187 Reward:  8.303180675862516\n",
      "Step:  188 Reward:  8.236271041486653\n",
      "Step:  189 Reward:  8.1212785325374\n",
      "Step:  190 Reward:  8.771484265448043\n",
      "Step:  191 Reward:  9.27445844092687\n",
      "Step:  192 Reward:  10.08940095103435\n",
      "Step:  193 Reward:  10.552174494893562\n",
      "Step:  194 Reward:  10.626812322405359\n",
      "Step:  195 Reward:  12.077462832001741\n",
      "Step:  196 Reward:  12.71826446960273\n",
      "Step:  197 Reward:  11.233435399735907\n",
      "Step:  198 Reward:  12.949185073552993\n",
      "Step:  199 Reward:  10.21657568283818\n",
      "Step:  200 Reward:  8.511971391966352\n",
      "Step:  201 Reward:  8.516587040615434\n",
      "Step:  202 Reward:  10.619978031931637\n",
      "Step:  203 Reward:  10.298941037332751\n",
      "Step:  204 Reward:  14.329289358474298\n",
      "Step:  205 Reward:  11.67058475735386\n",
      "Step:  206 Reward:  12.231392036077454\n",
      "Step:  207 Reward:  12.050398988486492\n",
      "Step:  208 Reward:  11.695639979875978\n",
      "Step:  209 Reward:  10.687817233992865\n",
      "Step:  210 Reward:  10.078633853757642\n",
      "Step:  211 Reward:  12.278794611010785\n",
      "Step:  212 Reward:  10.058432707354843\n",
      "Step:  213 Reward:  12.72690163201022\n",
      "Step:  214 Reward:  10.144453605966692\n",
      "Step:  215 Reward:  10.256547780445397\n",
      "Step:  216 Reward:  9.87497603507009\n",
      "Step:  217 Reward:  10.528754254889295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  218 Reward:  10.388106388092616\n",
      "Step:  219 Reward:  11.823639200851623\n",
      "Step:  220 Reward:  10.95469222514039\n",
      "Step:  221 Reward:  11.862585806485246\n",
      "Step:  222 Reward:  10.076116092107862\n",
      "Step:  223 Reward:  8.781296957015527\n",
      "Step:  224 Reward:  11.005192637225619\n",
      "Step:  225 Reward:  9.569805977601522\n",
      "Step:  226 Reward:  10.297250523721202\n",
      "Step:  227 Reward:  9.606115799690855\n",
      "Step:  228 Reward:  9.94702147186791\n",
      "Step:  229 Reward:  9.456330663307533\n",
      "Step:  230 Reward:  9.384687600080646\n",
      "Step:  231 Reward:  10.581306421506945\n",
      "Step:  232 Reward:  8.51453925220397\n",
      "Step:  233 Reward:  11.135745582038199\n",
      "Step:  234 Reward:  11.756716155874196\n",
      "Step:  235 Reward:  13.243532529536262\n",
      "Step:  236 Reward:  14.375274265767816\n",
      "Step:  237 Reward:  13.39892490185419\n",
      "Step:  238 Reward:  12.115612531765928\n",
      "Step:  239 Reward:  10.220966168378226\n",
      "Step:  240 Reward:  9.130378672133194\n",
      "Step:  241 Reward:  11.349093594738056\n",
      "Step:  242 Reward:  8.950868095893288\n",
      "Step:  243 Reward:  10.050049527753812\n",
      "Step:  244 Reward:  11.987367371722584\n",
      "Step:  245 Reward:  12.041547926118291\n",
      "Step:  246 Reward:  12.24651635652811\n",
      "Step:  247 Reward:  10.450903058011221\n",
      "Step:  248 Reward:  11.103257560749265\n",
      "Step:  249 Reward:  12.05887976324377\n",
      "Step:  250 Reward:  10.007446697791593\n",
      "Step:  251 Reward:  10.450550930238094\n",
      "Step:  252 Reward:  10.531680849317397\n",
      "Step:  253 Reward:  10.380278248833811\n",
      "Step:  254 Reward:  9.107515728066815\n",
      "Step:  255 Reward:  9.712509242221653\n",
      "Step:  256 Reward:  10.966851276654532\n",
      "Step:  257 Reward:  11.2526328059136\n",
      "Step:  258 Reward:  9.695095706145395\n",
      "Step:  259 Reward:  11.937414011129787\n",
      "Step:  260 Reward:  13.292911683427327\n",
      "Step:  261 Reward:  11.568292764017167\n",
      "Step:  262 Reward:  11.18006540383414\n",
      "Step:  263 Reward:  10.742530737236846\n",
      "Step:  264 Reward:  11.50373112630865\n",
      "Step:  265 Reward:  8.579907117908514\n",
      "Step:  266 Reward:  11.501812629349548\n",
      "Step:  267 Reward:  9.536911355530114\n",
      "Step:  268 Reward:  8.119054019235021\n",
      "Step:  269 Reward:  10.514602345784844\n",
      "Step:  270 Reward:  10.51682222141041\n",
      "Step:  271 Reward:  11.480097563128266\n",
      "Step:  272 Reward:  10.391488981552676\n",
      "Step:  273 Reward:  12.938311223450102\n",
      "Step:  274 Reward:  11.540515824909878\n",
      "Step:  275 Reward:  9.768939498372987\n",
      "Step:  276 Reward:  5.258673128763942\n",
      "Step:  277 Reward:  10.717729223659148\n",
      "Step:  278 Reward:  11.633521635724277\n",
      "Step:  279 Reward:  10.324507170095597\n",
      "Step:  280 Reward:  9.92299264721314\n",
      "Step:  281 Reward:  11.9294799394992\n",
      "Step:  282 Reward:  11.710784647133995\n",
      "Step:  283 Reward:  12.010355247475998\n",
      "Step:  284 Reward:  12.105038066868214\n",
      "Step:  285 Reward:  9.657866846001678\n",
      "Step:  286 Reward:  11.424664878649509\n",
      "Step:  287 Reward:  10.27075931157202\n",
      "Step:  288 Reward:  11.869730246498326\n",
      "Step:  289 Reward:  11.191924653262008\n",
      "Step:  290 Reward:  11.143229865028665\n",
      "Step:  291 Reward:  11.372288257908222\n",
      "Step:  292 Reward:  10.692671536074878\n",
      "Step:  293 Reward:  8.202508807660296\n",
      "Step:  294 Reward:  11.455519166216288\n",
      "Step:  295 Reward:  9.51942909149115\n",
      "Step:  296 Reward:  10.960880787581152\n",
      "Step:  297 Reward:  10.962836507027653\n",
      "Step:  298 Reward:  13.118039159455487\n",
      "Step:  299 Reward:  10.849406924323654\n",
      "Step:  300 Reward:  28.396302161408574\n",
      "Step:  301 Reward:  13.343064215503407\n",
      "Step:  302 Reward:  29.107702593362045\n",
      "Step:  303 Reward:  30.585579455815058\n",
      "Step:  304 Reward:  25.731622084019527\n",
      "Step:  305 Reward:  62.01058325609146\n",
      "Step:  306 Reward:  51.04481374015909\n",
      "Step:  307 Reward:  54.379285063606865\n",
      "Step:  308 Reward:  52.90141309534029\n",
      "Step:  309 Reward:  31.888488635678357\n",
      "Step:  310 Reward:  64.83011706689405\n",
      "Step:  311 Reward:  15.363779416050537\n",
      "Step:  312 Reward:  56.07021845421532\n",
      "Step:  313 Reward:  10.630620138917267\n",
      "Step:  314 Reward:  30.64406425338912\n",
      "Step:  315 Reward:  15.099598874233386\n",
      "Step:  316 Reward:  109.35088623068411\n",
      "Step:  317 Reward:  197.08355441117143\n",
      "Step:  318 Reward:  92.06313952058349\n",
      "Step:  319 Reward:  115.55325891164497\n",
      "Step:  320 Reward:  34.96534594124255\n",
      "Step:  321 Reward:  20.274994634448195\n",
      "Step:  322 Reward:  -1.0147183679987086\n",
      "Step:  323 Reward:  32.49637290507533\n",
      "Step:  324 Reward:  145.60365427620764\n",
      "Step:  325 Reward:  128.89972456679186\n",
      "Step:  326 Reward:  82.01447339994407\n",
      "Step:  327 Reward:  57.6915644675214\n",
      "Step:  328 Reward:  170.74228614037946\n",
      "Step:  329 Reward:  73.36859319467666\n",
      "Step:  330 Reward:  105.78564029000488\n",
      "Step:  331 Reward:  36.97094643126235\n",
      "Step:  332 Reward:  64.3953541143189\n",
      "Step:  333 Reward:  32.19800138060309\n",
      "Step:  334 Reward:  17.939482345379172\n",
      "Step:  335 Reward:  63.884723057678684\n",
      "Step:  336 Reward:  80.82310942734634\n",
      "Step:  337 Reward:  221.55484330072338\n",
      "Step:  338 Reward:  47.859401814469415\n",
      "Step:  339 Reward:  25.35895408931258\n",
      "Step:  340 Reward:  219.08034952251214\n",
      "Step:  341 Reward:  56.636498469436404\n",
      "Step:  342 Reward:  129.47052078934604\n",
      "Step:  343 Reward:  18.320661916344914\n",
      "Step:  344 Reward:  58.73146295097014\n",
      "Step:  345 Reward:  57.772148787805236\n",
      "Step:  346 Reward:  158.57199057220333\n",
      "Step:  347 Reward:  36.12706287579873\n",
      "Step:  348 Reward:  55.73471577966802\n",
      "Step:  349 Reward:  -9.135754086767474\n",
      "Step:  350 Reward:  59.19980886732543\n",
      "Step:  351 Reward:  124.75542073918521\n",
      "Step:  352 Reward:  35.39178632108426\n",
      "Step:  353 Reward:  34.312628139250194\n",
      "Step:  354 Reward:  187.18097269116583\n",
      "Step:  355 Reward:  158.4222239384444\n",
      "Step:  356 Reward:  95.5354971091317\n",
      "Step:  357 Reward:  52.06348534234772\n",
      "Step:  358 Reward:  149.34701409392426\n",
      "Step:  359 Reward:  87.93332791417329\n",
      "Step:  360 Reward:  73.8160197617037\n",
      "Step:  361 Reward:  45.27609592973254\n",
      "Step:  362 Reward:  49.93172448754222\n",
      "Step:  363 Reward:  122.12045464636995\n",
      "Step:  364 Reward:  46.88707236776354\n",
      "Step:  365 Reward:  20.601978441271147\n",
      "Step:  366 Reward:  30.437090457433904\n",
      "Step:  367 Reward:  239.99352796059742\n",
      "Step:  368 Reward:  36.61376700501052\n",
      "Step:  369 Reward:  266.9830897733789\n",
      "Step:  370 Reward:  69.05346691769853\n",
      "Step:  371 Reward:  31.142452586666423\n",
      "Step:  372 Reward:  57.36743952628853\n",
      "Step:  373 Reward:  121.60818371482091\n",
      "Step:  374 Reward:  49.390364944940586\n",
      "Step:  375 Reward:  175.63601335230334\n",
      "Step:  376 Reward:  100.00355592245535\n",
      "Step:  377 Reward:  182.46983231213642\n",
      "Step:  378 Reward:  65.42524162214815\n",
      "Step:  379 Reward:  39.20719886762038\n",
      "Step:  380 Reward:  39.61276695828622\n",
      "Step:  381 Reward:  113.47437519158908\n",
      "Step:  382 Reward:  189.4676720539638\n",
      "Step:  383 Reward:  78.4651035470581\n",
      "Step:  384 Reward:  41.758933684439526\n",
      "Step:  385 Reward:  127.70778370650194\n",
      "Step:  386 Reward:  156.5702156461143\n",
      "Step:  387 Reward:  117.29145843054772\n",
      "Step:  388 Reward:  189.65530819355138\n",
      "Step:  389 Reward:  190.4488710462441\n",
      "Step:  390 Reward:  45.824792616639506\n",
      "Step:  391 Reward:  175.56275677749207\n",
      "Step:  392 Reward:  172.22747630333308\n",
      "Step:  393 Reward:  192.4881117212942\n",
      "Step:  394 Reward:  193.6344478765219\n",
      "Step:  395 Reward:  76.89764119281715\n",
      "Step:  396 Reward:  204.72961484911173\n",
      "Step:  397 Reward:  191.57993962951392\n",
      "Step:  398 Reward:  200.10663992980747\n",
      "Step:  399 Reward:  177.91590874637058\n",
      "Step:  400 Reward:  215.68895500773942\n",
      "Step:  401 Reward:  212.21633087244638\n",
      "Step:  402 Reward:  219.6205883370357\n",
      "Step:  403 Reward:  218.14254516123557\n",
      "Step:  404 Reward:  214.89691692190638\n",
      "Step:  405 Reward:  224.74424771500347\n",
      "Step:  406 Reward:  180.0517850873213\n",
      "Step:  407 Reward:  192.8305346203926\n",
      "Step:  408 Reward:  82.1138565761716\n",
      "Step:  409 Reward:  224.52217617728684\n",
      "Step:  410 Reward:  230.959043992228\n",
      "Step:  411 Reward:  181.9719284010895\n",
      "Step:  412 Reward:  226.1413097733424\n",
      "Step:  413 Reward:  231.1254749661274\n",
      "Step:  414 Reward:  240.50888421780846\n",
      "Step:  415 Reward:  228.83529563494315\n",
      "Step:  416 Reward:  244.43977191835705\n",
      "Step:  417 Reward:  230.99544531763138\n",
      "Step:  418 Reward:  235.9538707858265\n",
      "Step:  419 Reward:  235.95923430958464\n",
      "Step:  420 Reward:  225.85393002166757\n",
      "Step:  421 Reward:  226.3414014803564\n",
      "Step:  422 Reward:  135.92250169962043\n",
      "Step:  423 Reward:  234.99531292262157\n",
      "Step:  424 Reward:  224.45696651925795\n",
      "Step:  425 Reward:  249.70864298022568\n",
      "Step:  426 Reward:  241.103505516112\n",
      "Step:  427 Reward:  249.8461132582954\n",
      "Step:  428 Reward:  217.43859654036913\n",
      "Step:  429 Reward:  236.2391527232024\n",
      "Step:  430 Reward:  53.89852080025844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  431 Reward:  60.21360385019215\n",
      "Step:  432 Reward:  228.5769552544754\n",
      "Step:  433 Reward:  237.0667971333898\n",
      "Step:  434 Reward:  231.69974338292147\n",
      "Step:  435 Reward:  230.54481316636264\n",
      "Step:  436 Reward:  188.0104504693506\n",
      "Step:  437 Reward:  131.4233419998278\n",
      "Step:  438 Reward:  232.54766627190745\n",
      "Step:  439 Reward:  245.32803749486615\n",
      "Step:  440 Reward:  246.92270085973354\n",
      "Step:  441 Reward:  102.08504680956176\n",
      "Step:  442 Reward:  209.68956569649373\n",
      "Step:  443 Reward:  249.38164980023592\n",
      "Step:  444 Reward:  249.1000471436739\n",
      "Step:  445 Reward:  238.85877516154142\n",
      "Step:  446 Reward:  240.30674329386568\n",
      "Step:  447 Reward:  244.18594709796807\n",
      "Step:  448 Reward:  225.7375956610299\n",
      "Step:  449 Reward:  216.62790299425902\n",
      "Step:  450 Reward:  235.45126288649504\n",
      "Step:  451 Reward:  114.3491331228251\n",
      "Step:  452 Reward:  265.2244817757962\n",
      "Step:  453 Reward:  277.29411381384745\n",
      "Step:  454 Reward:  174.33626152803936\n",
      "Step:  455 Reward:  259.9154197749472\n",
      "Step:  456 Reward:  252.5303252650397\n",
      "Step:  457 Reward:  249.64619648563428\n",
      "Step:  458 Reward:  262.71209862727125\n",
      "Step:  459 Reward:  61.191018055317144\n",
      "Step:  460 Reward:  267.1187379078906\n",
      "Step:  461 Reward:  276.61478835410327\n",
      "Step:  462 Reward:  245.73868961215595\n",
      "Step:  463 Reward:  253.18197689120495\n",
      "Step:  464 Reward:  253.5325283074611\n",
      "Step:  465 Reward:  147.6690552502524\n",
      "Step:  466 Reward:  245.14986547597448\n",
      "Step:  467 Reward:  254.24081233273367\n",
      "Step:  468 Reward:  235.4622838760092\n",
      "Step:  469 Reward:  245.4176396564796\n",
      "Step:  470 Reward:  254.12519465454395\n",
      "Step:  471 Reward:  269.0875187477675\n",
      "Step:  472 Reward:  275.7614095911859\n",
      "Step:  473 Reward:  279.0546202125807\n",
      "Step:  474 Reward:  268.68014634116696\n",
      "Step:  475 Reward:  276.27081388163856\n",
      "Step:  476 Reward:  251.14876285172284\n",
      "Step:  477 Reward:  259.8795855132405\n",
      "Step:  478 Reward:  270.9969018978585\n",
      "Step:  479 Reward:  192.0456681861912\n",
      "Step:  480 Reward:  276.23286728154324\n",
      "Step:  481 Reward:  268.2725248927064\n",
      "Step:  482 Reward:  280.1795971295487\n",
      "Step:  483 Reward:  271.7933412391228\n",
      "Step:  484 Reward:  282.7616681880869\n",
      "Step:  485 Reward:  280.53019353267524\n",
      "Step:  486 Reward:  266.52580731461813\n",
      "Step:  487 Reward:  286.89715753868654\n",
      "Step:  488 Reward:  265.6157309836133\n",
      "Step:  489 Reward:  270.23973265092803\n",
      "Step:  490 Reward:  271.157802274903\n",
      "Step:  491 Reward:  251.3400795710307\n",
      "Step:  492 Reward:  251.33240802245973\n",
      "Step:  493 Reward:  283.1969540956635\n",
      "Step:  494 Reward:  268.77051456261927\n",
      "Step:  495 Reward:  273.90153972987866\n",
      "Step:  496 Reward:  273.9370488513527\n",
      "Step:  497 Reward:  288.4910757151481\n",
      "Step:  498 Reward:  272.85202016137896\n",
      "Step:  499 Reward:  265.18394331293695\n",
      "Step:  500 Reward:  261.91050637636647\n",
      "Step:  501 Reward:  273.59530340513345\n",
      "Step:  502 Reward:  271.1256255243499\n",
      "Step:  503 Reward:  268.89680707991926\n",
      "Step:  504 Reward:  259.4880590274701\n",
      "Step:  505 Reward:  265.07616473285805\n",
      "Step:  506 Reward:  286.5386720077671\n",
      "Step:  507 Reward:  283.2984967934144\n",
      "Step:  508 Reward:  278.3273852818123\n",
      "Step:  509 Reward:  284.8162720938785\n",
      "Step:  510 Reward:  290.60371479828035\n",
      "Step:  511 Reward:  286.0799869495865\n",
      "Step:  512 Reward:  28.829127974591128\n",
      "Step:  513 Reward:  260.9525498400119\n",
      "Step:  514 Reward:  266.50818517005894\n",
      "Step:  515 Reward:  277.8105802750847\n",
      "Step:  516 Reward:  251.35227231845093\n",
      "Step:  517 Reward:  266.2991821099375\n",
      "Step:  518 Reward:  272.18360999458093\n",
      "Step:  519 Reward:  286.5235055010269\n",
      "Step:  520 Reward:  270.3421459831605\n",
      "Step:  521 Reward:  263.0133543621573\n",
      "Step:  522 Reward:  258.5715285781689\n",
      "Step:  523 Reward:  277.6950607228577\n",
      "Step:  524 Reward:  285.90393469699984\n",
      "Step:  525 Reward:  273.0331917978804\n",
      "Step:  526 Reward:  268.92390838785184\n",
      "Step:  527 Reward:  281.0422799425454\n",
      "Step:  528 Reward:  289.0438365197519\n",
      "Step:  529 Reward:  273.7144200567165\n",
      "Step:  530 Reward:  279.38613204048426\n",
      "Step:  531 Reward:  287.27375900015477\n",
      "Step:  532 Reward:  291.4074403156675\n",
      "Step:  533 Reward:  233.15369487898732\n",
      "Step:  534 Reward:  290.3169509050299\n",
      "Step:  535 Reward:  291.5519797604215\n",
      "Step:  536 Reward:  289.20252720451606\n",
      "Step:  537 Reward:  287.6849661861138\n",
      "Step:  538 Reward:  287.8225016894635\n",
      "Step:  539 Reward:  285.51932251757285\n",
      "Step:  540 Reward:  291.24857075321387\n",
      "Step:  541 Reward:  291.6584841594861\n",
      "Step:  542 Reward:  291.1367102590985\n",
      "Step:  543 Reward:  290.96399374649934\n",
      "Step:  544 Reward:  294.43406463159886\n",
      "Step:  545 Reward:  289.44802452358005\n",
      "Step:  546 Reward:  290.33152286244336\n",
      "Step:  547 Reward:  276.34282146746074\n",
      "Step:  548 Reward:  290.9252516812736\n",
      "Step:  549 Reward:  274.813528581549\n",
      "Step:  550 Reward:  288.2270846861079\n",
      "Step:  551 Reward:  290.22546469542704\n",
      "Step:  552 Reward:  272.9995395604952\n",
      "Step:  553 Reward:  293.7336277749212\n",
      "Step:  554 Reward:  290.82305903458763\n",
      "Step:  555 Reward:  290.0236261153938\n",
      "Step:  556 Reward:  290.93997462877655\n",
      "Step:  557 Reward:  288.6814160159217\n",
      "Step:  558 Reward:  291.06782372536463\n",
      "Step:  559 Reward:  283.7911810464464\n",
      "Step:  560 Reward:  287.4342405432827\n",
      "Step:  561 Reward:  292.3685829744494\n",
      "Step:  562 Reward:  291.282847529652\n",
      "Step:  563 Reward:  285.20292200145184\n",
      "Step:  564 Reward:  283.94974851559596\n",
      "Step:  565 Reward:  272.6189264722521\n",
      "Step:  566 Reward:  291.7345856671627\n",
      "Step:  567 Reward:  286.7448095298272\n",
      "Step:  568 Reward:  290.4886468806498\n",
      "Step:  569 Reward:  290.11264902391997\n",
      "Step:  570 Reward:  290.20947098842163\n",
      "Step:  571 Reward:  292.52937483765777\n",
      "Step:  572 Reward:  289.91343978429126\n",
      "Step:  573 Reward:  291.5676795240167\n",
      "Step:  574 Reward:  294.0313153856475\n",
      "Step:  575 Reward:  289.47536830154917\n",
      "Step:  576 Reward:  288.83132076801047\n",
      "Step:  577 Reward:  294.0383221242354\n",
      "Step:  578 Reward:  289.29139398549637\n",
      "Step:  579 Reward:  92.67536162751165\n",
      "Step:  580 Reward:  278.27399451276386\n",
      "Step:  581 Reward:  288.51729314529314\n",
      "Step:  582 Reward:  292.0861023577896\n",
      "Step:  583 Reward:  288.2530200387069\n",
      "Step:  584 Reward:  274.08493584778887\n",
      "Step:  585 Reward:  269.3039571685824\n",
      "Step:  586 Reward:  289.97411059867034\n",
      "Step:  587 Reward:  290.56536475549143\n",
      "Step:  588 Reward:  281.8882799198017\n",
      "Step:  589 Reward:  288.7006147595578\n",
      "Step:  590 Reward:  287.02518553684547\n",
      "Step:  591 Reward:  287.99153041706177\n",
      "Step:  592 Reward:  288.2549319820998\n",
      "Step:  593 Reward:  292.6699163523026\n",
      "Step:  594 Reward:  291.7083818392473\n",
      "Step:  595 Reward:  290.79248136620424\n",
      "Step:  596 Reward:  283.9307372655655\n",
      "Step:  597 Reward:  286.81709593609173\n",
      "Step:  598 Reward:  290.49099306058406\n",
      "Step:  599 Reward:  289.935093924273\n",
      "Step:  600 Reward:  277.2337965178755\n",
      "Step:  601 Reward:  290.000947384393\n",
      "Step:  602 Reward:  287.87354598150154\n",
      "Step:  603 Reward:  287.4935207523374\n",
      "Step:  604 Reward:  287.36683387080075\n",
      "Step:  605 Reward:  289.3441850595288\n",
      "Step:  606 Reward:  292.47549481054256\n",
      "Step:  607 Reward:  289.7644535011341\n",
      "Step:  608 Reward:  287.13534621171243\n",
      "Step:  609 Reward:  287.0537702917076\n",
      "Step:  610 Reward:  284.5880120776075\n",
      "Step:  611 Reward:  289.11559861479805\n",
      "Step:  612 Reward:  287.7458756139181\n",
      "Step:  613 Reward:  290.92259995363645\n",
      "Step:  614 Reward:  289.2245895123082\n",
      "Step:  615 Reward:  290.59174378034254\n",
      "Step:  616 Reward:  289.371317593979\n",
      "Step:  617 Reward:  288.37924673936476\n",
      "Step:  618 Reward:  290.1316767378872\n",
      "Step:  619 Reward:  290.0586549235764\n",
      "Step:  620 Reward:  287.91953564850314\n",
      "Step:  621 Reward:  288.5142852164375\n",
      "Step:  622 Reward:  286.78896803435384\n",
      "Step:  623 Reward:  288.8772942226725\n",
      "Step:  624 Reward:  292.80720200491896\n",
      "Step:  625 Reward:  286.40757699691954\n",
      "Step:  626 Reward:  286.5814249796394\n",
      "Step:  627 Reward:  286.77953834370925\n",
      "Step:  628 Reward:  281.8412596602519\n",
      "Step:  629 Reward:  287.4596535336734\n",
      "Step:  630 Reward:  288.65831442535665\n",
      "Step:  631 Reward:  285.9342777902502\n",
      "Step:  632 Reward:  277.4558013297067\n",
      "Step:  633 Reward:  287.2633621234744\n",
      "Step:  634 Reward:  291.23154389073454\n",
      "Step:  635 Reward:  289.0988638464216\n",
      "Step:  636 Reward:  290.56414651753715\n",
      "Step:  637 Reward:  290.2100902812056\n",
      "Step:  638 Reward:  292.1845866087923\n",
      "Step:  639 Reward:  289.29356309653696\n",
      "Step:  640 Reward:  289.0222137366735\n",
      "Step:  641 Reward:  293.0798475187182\n",
      "Step:  642 Reward:  291.62730534527225\n",
      "Step:  643 Reward:  285.7235349245384\n",
      "Step:  644 Reward:  288.3279669870781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  645 Reward:  289.6701627961298\n",
      "Step:  646 Reward:  292.6090675243363\n",
      "Step:  647 Reward:  291.4154155182841\n",
      "Step:  648 Reward:  283.4990474101051\n",
      "Step:  649 Reward:  289.3130716200317\n",
      "Step:  650 Reward:  292.60474113672643\n",
      "Step:  651 Reward:  291.3077820780503\n",
      "Step:  652 Reward:  291.3916715427709\n",
      "Step:  653 Reward:  292.6167764912496\n",
      "Step:  654 Reward:  292.072852386178\n",
      "Step:  655 Reward:  293.80153479089216\n",
      "Step:  656 Reward:  292.26907170739315\n",
      "Step:  657 Reward:  294.44542860978356\n",
      "Step:  658 Reward:  120.78659668954846\n",
      "Step:  659 Reward:  289.5363347208893\n",
      "Step:  660 Reward:  292.1509220797319\n",
      "Step:  661 Reward:  289.8674394335326\n",
      "Step:  662 Reward:  286.0142107372029\n",
      "Step:  663 Reward:  291.7686056037756\n",
      "Step:  664 Reward:  290.8898126652285\n",
      "Step:  665 Reward:  289.06337454009594\n",
      "Step:  666 Reward:  289.2951369707579\n",
      "Step:  667 Reward:  292.8812687437367\n",
      "Step:  668 Reward:  291.130918644188\n",
      "Step:  669 Reward:  287.2646953154002\n",
      "Step:  670 Reward:  281.4308666700308\n",
      "Step:  671 Reward:  288.52855468461314\n",
      "Step:  672 Reward:  292.5324665675301\n",
      "Step:  673 Reward:  289.83159838638545\n",
      "Step:  674 Reward:  210.44954504648257\n",
      "Step:  675 Reward:  288.3905310307758\n",
      "Step:  676 Reward:  138.9963081004046\n",
      "Step:  677 Reward:  285.4835302130321\n",
      "Step:  678 Reward:  288.24714310913845\n",
      "Step:  679 Reward:  284.75513084095087\n",
      "Step:  680 Reward:  286.6463609667137\n",
      "Step:  681 Reward:  289.4927096970081\n",
      "Step:  682 Reward:  285.21723008897305\n",
      "Step:  683 Reward:  287.47553610400934\n",
      "Step:  684 Reward:  290.4272131836989\n",
      "Step:  685 Reward:  290.8177402106276\n",
      "Step:  686 Reward:  288.4569715455592\n",
      "Step:  687 Reward:  289.3768399650813\n",
      "Step:  688 Reward:  291.68805044332623\n",
      "Step:  689 Reward:  288.31367797376504\n",
      "Step:  690 Reward:  282.0026953492504\n",
      "Step:  691 Reward:  288.7312644644169\n",
      "Step:  692 Reward:  288.4253989914259\n",
      "Step:  693 Reward:  289.4104342606841\n",
      "Step:  694 Reward:  290.00140376625586\n",
      "Step:  695 Reward:  288.77991574745744\n",
      "Step:  696 Reward:  288.3018387141098\n",
      "Step:  697 Reward:  287.08763943959104\n",
      "Step:  698 Reward:  285.68147636015766\n",
      "Step:  699 Reward:  289.38354756343733\n",
      "Step:  700 Reward:  290.69268055953063\n",
      "Step:  701 Reward:  288.3652535776503\n",
      "Step:  702 Reward:  289.38053323263466\n",
      "Step:  703 Reward:  291.436673236565\n",
      "Step:  704 Reward:  291.87258134535267\n",
      "Step:  705 Reward:  285.9847789471292\n",
      "Step:  706 Reward:  292.3844332878802\n",
      "Step:  707 Reward:  285.66218173931463\n",
      "Step:  708 Reward:  291.5113433506288\n",
      "Step:  709 Reward:  291.15512361247056\n",
      "Step:  710 Reward:  288.3116565152463\n",
      "Step:  711 Reward:  292.04074187699035\n",
      "Step:  712 Reward:  293.7844431083283\n",
      "Step:  713 Reward:  291.54488091818945\n",
      "Step:  714 Reward:  291.7959117449778\n",
      "Step:  715 Reward:  289.9285607081487\n",
      "Step:  716 Reward:  293.7717951648531\n",
      "Step:  717 Reward:  295.3491432036583\n",
      "Step:  718 Reward:  291.7968855835273\n",
      "Step:  719 Reward:  291.3696474010601\n",
      "Step:  720 Reward:  291.254397082653\n",
      "Step:  721 Reward:  293.8346747791902\n",
      "Step:  722 Reward:  291.9023653271611\n",
      "Step:  723 Reward:  291.9022302933212\n",
      "Step:  724 Reward:  291.43663699380124\n",
      "Step:  725 Reward:  292.78417514716085\n",
      "Step:  726 Reward:  291.38879622980363\n",
      "Step:  727 Reward:  292.2416468066909\n",
      "Step:  728 Reward:  288.21143844516604\n",
      "Step:  729 Reward:  290.39381708594703\n",
      "Step:  730 Reward:  292.068960832943\n",
      "Step:  731 Reward:  292.2086454644506\n",
      "Step:  732 Reward:  292.84846389533095\n",
      "Step:  733 Reward:  294.61772344700876\n",
      "Step:  734 Reward:  295.66928646108295\n",
      "Step:  735 Reward:  292.05751944438623\n",
      "Step:  736 Reward:  294.4779530786709\n",
      "Step:  737 Reward:  297.6448167450681\n",
      "Step:  738 Reward:  292.2536302381095\n",
      "Step:  739 Reward:  290.7824917912533\n",
      "Step:  740 Reward:  294.1529799584594\n",
      "Step:  741 Reward:  293.8181983987856\n",
      "Step:  742 Reward:  295.43824974728943\n",
      "Step:  743 Reward:  290.2255133574302\n",
      "Step:  744 Reward:  289.98981800256496\n",
      "Step:  745 Reward:  292.719864666681\n",
      "Step:  746 Reward:  293.3268798384176\n",
      "Step:  747 Reward:  292.36793994252827\n",
      "Step:  748 Reward:  279.17921573383353\n",
      "Step:  749 Reward:  295.73391813437286\n",
      "Step:  750 Reward:  296.6788292077017\n",
      "Step:  751 Reward:  294.12427386887146\n",
      "Step:  752 Reward:  293.53600695564455\n",
      "Step:  753 Reward:  295.07721098756537\n",
      "Step:  754 Reward:  297.36225611799074\n",
      "Step:  755 Reward:  298.4837943340908\n",
      "Step:  756 Reward:  297.16146832357873\n",
      "Step:  757 Reward:  295.87764178287387\n",
      "Step:  758 Reward:  297.264069712604\n",
      "Step:  759 Reward:  296.63674485893455\n",
      "Step:  760 Reward:  296.483874371809\n",
      "Step:  761 Reward:  295.1603856973811\n",
      "Step:  762 Reward:  299.39874921274736\n",
      "Step:  763 Reward:  294.5064823259032\n",
      "Step:  764 Reward:  299.28271538778347\n",
      "Step:  765 Reward:  298.4620729332127\n",
      "Step:  766 Reward:  298.58138710754395\n",
      "Step:  767 Reward:  296.4451135973637\n",
      "Step:  768 Reward:  296.87846743702227\n",
      "Step:  769 Reward:  294.90280949268197\n",
      "Step:  770 Reward:  296.76119298211523\n",
      "Step:  771 Reward:  298.07136343107516\n",
      "Step:  772 Reward:  298.8634947423795\n",
      "Step:  773 Reward:  299.08190351053565\n",
      "Step:  774 Reward:  295.8148766194905\n",
      "Step:  775 Reward:  294.9651754436093\n",
      "Step:  776 Reward:  295.9470994031998\n",
      "Step:  777 Reward:  298.3216069093805\n",
      "Step:  778 Reward:  295.3218761099829\n",
      "Step:  779 Reward:  295.5878293478512\n",
      "Step:  780 Reward:  296.5163238846047\n",
      "Step:  781 Reward:  297.5152341861506\n",
      "Step:  782 Reward:  303.78185743504946\n",
      "Step:  783 Reward:  299.63847187110775\n",
      "Step:  784 Reward:  299.0358621957802\n",
      "Step:  785 Reward:  301.6662653363693\n",
      "Step:  786 Reward:  301.43541885180855\n",
      "Step:  787 Reward:  300.41226793371186\n",
      "Step:  788 Reward:  301.01785079297866\n",
      "Step:  789 Reward:  302.400009692485\n",
      "Step:  790 Reward:  301.5043526856201\n",
      "Step:  791 Reward:  299.45293585457404\n",
      "Step:  792 Reward:  295.8650614852419\n",
      "Step:  793 Reward:  305.1762675735686\n",
      "Step:  794 Reward:  304.1604055949912\n",
      "Step:  795 Reward:  302.45702743384857\n",
      "Step:  796 Reward:  302.581969969118\n",
      "Step:  797 Reward:  298.38396421224826\n",
      "Step:  798 Reward:  306.0335843224596\n",
      "Step:  799 Reward:  303.79663106076214\n",
      "Step:  800 Reward:  299.88383574228675\n",
      "Step:  801 Reward:  299.7743778361577\n",
      "Step:  802 Reward:  297.7817226802542\n",
      "Step:  803 Reward:  296.48197265392355\n",
      "Step:  804 Reward:  302.60181761673186\n",
      "Step:  805 Reward:  303.7958080859694\n",
      "Step:  806 Reward:  306.42736080302893\n",
      "Step:  807 Reward:  304.3334025007283\n",
      "Step:  808 Reward:  303.8719105905004\n",
      "Step:  809 Reward:  304.1281354684503\n",
      "Step:  810 Reward:  305.7293238831347\n",
      "Step:  811 Reward:  303.9105169926735\n",
      "Step:  812 Reward:  304.5390499826191\n",
      "Step:  813 Reward:  305.1150827489823\n",
      "Step:  814 Reward:  305.5114581630062\n",
      "Step:  815 Reward:  305.5243704667879\n",
      "Step:  816 Reward:  304.5842020444107\n",
      "Step:  817 Reward:  306.23566266606014\n",
      "Step:  818 Reward:  311.1429409255707\n",
      "Step:  819 Reward:  309.77046042726084\n",
      "Step:  820 Reward:  305.6467069184299\n",
      "Step:  821 Reward:  305.8354627321296\n",
      "Step:  822 Reward:  303.61953243644615\n",
      "Step:  823 Reward:  304.3379276349154\n",
      "Step:  824 Reward:  307.96295062176625\n",
      "Step:  825 Reward:  305.7158121291235\n",
      "Step:  826 Reward:  306.05576516284043\n",
      "Step:  827 Reward:  308.7883642974571\n",
      "Step:  828 Reward:  309.3244369822748\n",
      "Step:  829 Reward:  107.96928751654158\n",
      "Step:  830 Reward:  305.21038178499555\n",
      "Step:  831 Reward:  305.81742068137333\n",
      "Step:  832 Reward:  306.3483309711838\n",
      "Step:  833 Reward:  306.6776747120966\n",
      "Step:  834 Reward:  304.6669075849627\n",
      "Step:  835 Reward:  304.82292370147366\n",
      "Step:  836 Reward:  308.177431251616\n",
      "Step:  837 Reward:  304.8422069272635\n",
      "Step:  838 Reward:  302.54040184007056\n",
      "Step:  839 Reward:  307.0286688569674\n",
      "Step:  840 Reward:  304.9618910639588\n",
      "Step:  841 Reward:  306.96820299994647\n",
      "Step:  842 Reward:  308.6441249670736\n",
      "Step:  843 Reward:  310.18578274145784\n",
      "Step:  844 Reward:  308.37571843986325\n",
      "Step:  845 Reward:  306.54409751757527\n",
      "Step:  846 Reward:  310.6324590284364\n",
      "Step:  847 Reward:  313.8316354138764\n",
      "Step:  848 Reward:  310.23880764058447\n",
      "Step:  849 Reward:  311.23449246370035\n",
      "Step:  850 Reward:  311.44068716995275\n",
      "Step:  851 Reward:  308.3732578610361\n",
      "Step:  852 Reward:  305.37229591435977\n",
      "Step:  853 Reward:  308.20036388300673\n",
      "Step:  854 Reward:  309.9769706034841\n",
      "Step:  855 Reward:  313.27085133452977\n",
      "Step:  856 Reward:  309.5429441049491\n",
      "Step:  857 Reward:  310.0114009923796\n",
      "Step:  858 Reward:  310.32589670768976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  859 Reward:  305.9644709593253\n",
      "Step:  860 Reward:  307.1572094123591\n",
      "Step:  861 Reward:  308.4353975151262\n",
      "Step:  862 Reward:  306.34233544654063\n",
      "Step:  863 Reward:  306.62627333446255\n",
      "Step:  864 Reward:  307.07067561544494\n",
      "Step:  865 Reward:  310.70949918243826\n",
      "Step:  866 Reward:  308.803012470193\n",
      "Step:  867 Reward:  311.0815136504547\n",
      "Step:  868 Reward:  310.898458531386\n",
      "Step:  869 Reward:  310.52112449558416\n",
      "Step:  870 Reward:  309.94624905624977\n",
      "Step:  871 Reward:  310.809152129623\n",
      "Step:  872 Reward:  311.1880188448004\n",
      "Step:  873 Reward:  310.446882924464\n",
      "Step:  874 Reward:  309.98147212616135\n",
      "Step:  875 Reward:  309.97139449276574\n",
      "Step:  876 Reward:  309.40632454469164\n",
      "Step:  877 Reward:  309.6277773431968\n",
      "Step:  878 Reward:  308.7200068724167\n",
      "Step:  879 Reward:  307.94695180683436\n",
      "Step:  880 Reward:  307.2604099081361\n",
      "Step:  881 Reward:  308.7492317009486\n",
      "Step:  882 Reward:  310.3033046409393\n",
      "Step:  883 Reward:  308.5706301090862\n",
      "Step:  884 Reward:  307.9040156976966\n",
      "Step:  885 Reward:  308.39018205855\n",
      "Step:  886 Reward:  310.2769801594735\n",
      "Step:  887 Reward:  309.78233457193846\n",
      "Step:  888 Reward:  310.53659648114916\n",
      "Step:  889 Reward:  308.4859567614907\n",
      "Step:  890 Reward:  308.74509920825517\n",
      "Step:  891 Reward:  305.12464415004166\n",
      "Step:  892 Reward:  305.27865933871874\n",
      "Step:  893 Reward:  308.70914228538317\n",
      "Step:  894 Reward:  309.3524044192536\n",
      "Step:  895 Reward:  312.80425658483927\n",
      "Step:  896 Reward:  307.20951660379995\n",
      "Step:  897 Reward:  306.0981994399851\n",
      "Step:  898 Reward:  307.8188782183256\n",
      "Step:  899 Reward:  308.48564766339183\n",
      "Step:  900 Reward:  106.02757764146547\n",
      "Step:  901 Reward:  309.8353945920666\n",
      "Step:  902 Reward:  305.9629595298659\n",
      "Step:  903 Reward:  308.4287339326448\n",
      "Step:  904 Reward:  308.9253494685183\n",
      "Step:  905 Reward:  306.36004971796183\n",
      "Step:  906 Reward:  305.40626901528293\n",
      "Step:  907 Reward:  304.74841914147123\n",
      "Step:  908 Reward:  304.2337610316091\n",
      "Step:  909 Reward:  305.58309068867385\n",
      "Step:  910 Reward:  306.4379604010369\n",
      "Step:  911 Reward:  306.50050566662435\n",
      "Step:  912 Reward:  306.69458815433575\n",
      "Step:  913 Reward:  309.40283127503096\n",
      "Step:  914 Reward:  307.7773195410507\n",
      "Step:  915 Reward:  307.62984698444035\n",
      "Step:  916 Reward:  307.5247910618936\n",
      "Step:  917 Reward:  306.93294363134527\n",
      "Step:  918 Reward:  305.6895899360715\n",
      "Step:  919 Reward:  303.5330150047527\n",
      "Step:  920 Reward:  308.35041531894524\n",
      "Step:  921 Reward:  306.73405223239007\n",
      "Step:  922 Reward:  311.21050866921547\n",
      "Step:  923 Reward:  307.14354070840204\n",
      "Step:  924 Reward:  307.11745501113757\n",
      "Step:  925 Reward:  307.74121087805986\n",
      "Step:  926 Reward:  310.3192705724312\n",
      "Step:  927 Reward:  306.04036629181417\n",
      "Step:  928 Reward:  307.9125778915642\n",
      "Step:  929 Reward:  310.5171603979348\n",
      "Step:  930 Reward:  310.72509202633887\n",
      "Step:  931 Reward:  308.1715939138486\n",
      "Step:  932 Reward:  309.4936697335668\n",
      "Step:  933 Reward:  310.6959331322349\n",
      "Step:  934 Reward:  306.60857000459237\n",
      "Step:  935 Reward:  306.7617633218826\n",
      "Step:  936 Reward:  308.80482556892605\n",
      "Step:  937 Reward:  313.1510470488909\n",
      "Step:  938 Reward:  286.5657433368361\n",
      "Step:  939 Reward:  308.6716344616901\n",
      "Step:  940 Reward:  313.1882462836174\n",
      "Step:  941 Reward:  311.70919924986083\n",
      "Step:  942 Reward:  306.49175152319174\n",
      "Step:  943 Reward:  306.7597407770618\n",
      "Step:  944 Reward:  310.8049366131394\n",
      "Step:  945 Reward:  311.6542212692348\n",
      "Step:  946 Reward:  311.24340522570077\n",
      "Step:  947 Reward:  308.6687642107351\n",
      "Step:  948 Reward:  311.9786888211093\n",
      "Step:  949 Reward:  312.24966766382255\n",
      "Step:  950 Reward:  310.3816118131741\n",
      "Step:  951 Reward:  311.5139406021899\n",
      "Step:  952 Reward:  310.92343003325936\n",
      "Step:  953 Reward:  308.2130273048746\n",
      "Step:  954 Reward:  309.1659945782532\n",
      "Step:  955 Reward:  312.58360065901707\n",
      "Step:  956 Reward:  311.3316268149511\n",
      "Step:  957 Reward:  310.12650900980805\n",
      "Step:  958 Reward:  308.8719660903324\n",
      "Step:  959 Reward:  311.86574756499846\n",
      "Step:  960 Reward:  309.77884549987544\n",
      "Step:  961 Reward:  312.65730226062925\n",
      "Step:  962 Reward:  310.62725154494\n",
      "Step:  963 Reward:  308.01159417756014\n",
      "Step:  964 Reward:  309.1373587355173\n",
      "Step:  965 Reward:  309.68372930237757\n",
      "Step:  966 Reward:  308.33495264210063\n",
      "Step:  967 Reward:  312.3659511186085\n",
      "Step:  968 Reward:  312.14488222140375\n",
      "Step:  969 Reward:  311.0996793934262\n",
      "Step:  970 Reward:  310.93241645087\n",
      "Step:  971 Reward:  309.52142488446356\n",
      "Step:  972 Reward:  310.4839280595881\n",
      "Step:  973 Reward:  308.12982402109026\n",
      "Step:  974 Reward:  308.2542130304972\n",
      "Step:  975 Reward:  308.7368628130614\n",
      "Step:  976 Reward:  306.8833032168013\n",
      "Step:  977 Reward:  311.8614534976471\n",
      "Step:  978 Reward:  311.2516059979804\n",
      "Step:  979 Reward:  305.68415610893675\n",
      "Step:  980 Reward:  306.28826949318744\n",
      "Step:  981 Reward:  311.6964135471695\n",
      "Step:  982 Reward:  311.12354711463314\n",
      "Step:  983 Reward:  309.17846133355783\n",
      "Step:  984 Reward:  308.76330689588633\n",
      "Step:  985 Reward:  313.01051550397744\n",
      "Step:  986 Reward:  89.06415185178847\n",
      "Step:  987 Reward:  312.4620392770981\n",
      "Step:  988 Reward:  310.56766221533064\n",
      "Step:  989 Reward:  311.95549683829034\n",
      "Step:  990 Reward:  308.56143298435535\n",
      "Step:  991 Reward:  313.54263007552527\n",
      "Step:  992 Reward:  313.2788214546157\n",
      "Step:  993 Reward:  312.54428480567793\n",
      "Step:  994 Reward:  313.90945659029404\n",
      "Step:  995 Reward:  312.2662719469618\n",
      "Step:  996 Reward:  311.45090358684496\n",
      "Step:  997 Reward:  310.62135824896296\n",
      "Step:  998 Reward:  307.15981462185533\n",
      "Step:  999 Reward:  311.44621325607704\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "ENV_NAME = 'BipedalWalker-v3'\n",
    "\n",
    "videos_dir = mkdir('.', 'videos')\n",
    "monitor_dir = mkdir(videos_dir, ENV_NAME)\n",
    "\n",
    "hp = HP(env_name=ENV_NAME)\n",
    "trainer = ARSTrainer(hp=hp, monitor_dir=monitor_dir)\n",
    "trainer.train()\n",
    "end = datetime.datetime.now()\n",
    "print(\"Time elapsed\": end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19773867, -0.43398089,  0.0753329 , -0.40455625, -0.78022785,\n",
       "        -0.47065727, -0.28752706,  0.02663993, -0.04235575, -0.00839982,\n",
       "        -0.04940817, -0.14592225,  0.11720849,  0.04220249,  0.09243848,\n",
       "         0.05408006,  0.217542  ,  0.13483803, -0.10166978, -0.32866835,\n",
       "        -0.00126649,  0.09081812,  0.06181933,  0.05200593],\n",
       "       [-0.06774746,  0.07975838,  0.11180235, -0.083668  ,  0.21467017,\n",
       "        -0.159638  , -0.11771604, -0.28212586, -0.10031455,  0.4626594 ,\n",
       "         0.47707995, -0.02464654,  0.14722706, -0.03351459, -0.09869358,\n",
       "        -0.04679419,  0.26850789, -0.22494934,  0.48149819, -0.41699818,\n",
       "        -0.27997625,  0.25551447, -0.261145  ,  0.06094136],\n",
       "       [-0.28416599,  0.01871607, -0.13579297,  0.40673224, -0.11451224,\n",
       "         0.01893984, -0.56593534, -0.11441946,  0.1603778 , -0.63110977,\n",
       "        -0.09323498,  0.01092114,  0.06647962, -0.11395431,  0.20635529,\n",
       "        -0.48134119, -0.12073207, -0.02296331, -0.23105415,  0.29713571,\n",
       "        -0.10748665,  0.02233215, -0.06541907,  0.05559063],\n",
       "       [ 0.24716027,  0.24578585, -0.15897411, -0.25435104,  0.24297   ,\n",
       "         0.18871955, -0.09838014,  0.12231298, -0.03587745, -0.28419632,\n",
       "        -0.37318322, -0.45613835, -0.33224005, -0.04004181, -0.13613522,\n",
       "        -0.54438399, -0.05580985, -0.25016605, -0.14118687, -0.0516655 ,\n",
       "        -0.19283703,  0.09819555, -0.00102822, -0.03006884]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.policy.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'169.254.86.131'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socket.gethostbyname(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from collections import deque\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as backend\n",
    "from threading import Thread\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "import carla\n",
    "\n",
    "\n",
    "SHOW_PREVIEW = False\n",
    "IM_WIDTH = 640\n",
    "IM_HEIGHT = 480\n",
    "SECONDS_PER_EPISODE = 10\n",
    "REPLAY_MEMORY_SIZE = 5_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000\n",
    "MINIBATCH_SIZE = 16\n",
    "PREDICTION_BATCH_SIZE = 1\n",
    "TRAINING_BATCH_SIZE = MINIBATCH_SIZE // 4\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "MODEL_NAME = \"Xception\"\n",
    "\n",
    "MEMORY_FRACTION = 0.4\n",
    "MIN_REWARD = -200\n",
    "\n",
    "EPISODES = 100\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.95 ## 0.9975 99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 10\n",
    "\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "\n",
    "class CarEnv:\n",
    "    SHOW_CAM = SHOW_PREVIEW\n",
    "    STEER_AMT = 1.0\n",
    "    im_width = IM_WIDTH\n",
    "    im_height = IM_HEIGHT\n",
    "    front_camera = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(2.0)\n",
    "        self.world = self.client.get_world()\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = self.blueprint_library.filter(\"model3\")[0]\n",
    "\n",
    "    def reset(self):\n",
    "        self.collision_hist = []\n",
    "        self.actor_list = []\n",
    "\n",
    "        self.transform = random.choice(self.world.get_map().get_spawn_points())\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, self.transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "\n",
    "        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        self.rgb_cam.set_attribute(\"image_size_x\", f\"{self.im_width}\")\n",
    "        self.rgb_cam.set_attribute(\"image_size_y\", f\"{self.im_height}\")\n",
    "        self.rgb_cam.set_attribute(\"fov\", f\"110\")\n",
    "\n",
    "        transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_img(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        time.sleep(4)\n",
    "\n",
    "        colsensor = self.blueprint_library.find(\"sensor.other.collision\")\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "\n",
    "        while self.front_camera is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        self.episode_start = time.time()\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "\n",
    "        return self.front_camera\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_img(self, image):\n",
    "        i = np.array(image.raw_data)\n",
    "        #print(i.shape)\n",
    "        i2 = i.reshape((self.im_height, self.im_width, 4))\n",
    "        i3 = i2[:, :, :3]\n",
    "        if self.SHOW_CAM:\n",
    "            cv2.imshow(\"\", i3)\n",
    "            cv2.waitKey(1)\n",
    "        self.front_camera = i3\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=-1*self.STEER_AMT))\n",
    "        elif action == 1:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer= 0))\n",
    "        elif action == 2:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=1*self.STEER_AMT))\n",
    "\n",
    "        v = self.vehicle.get_velocity()\n",
    "        kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "\n",
    "        if len(self.collision_hist) != 0:\n",
    "            done = True\n",
    "            reward = -200\n",
    "        elif kmh < 50:\n",
    "            done = False\n",
    "            reward = -1\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 1\n",
    "\n",
    "        if self.episode_start + SECONDS_PER_EPISODE < time.time():\n",
    "            done = True\n",
    "\n",
    "        return self.front_camera, reward, done, None\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{int(time.time())}\")\n",
    "        self.target_update_counter = 0\n",
    "        self.graph = tf.get_default_graph()\n",
    "\n",
    "        self.terminate = False\n",
    "        self.last_logged_episode = 0\n",
    "        self.training_initialized = False\n",
    "\n",
    "    def create_model(self):\n",
    "        base_model = Xception(weights=None, include_top=False, input_shape=(IM_HEIGHT, IM_WIDTH,3))\n",
    "\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "        predictions = Dense(3, activation=\"linear\")(x)\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (current_state, action, reward, new_state, done)\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        with self.graph.as_default():\n",
    "            current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        with self.graph.as_default():\n",
    "            future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        log_this_step = False\n",
    "        if self.tensorboard.step > self.last_logged_episode:\n",
    "            log_this_step = True\n",
    "            self.last_log_episode = self.tensorboard.step\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            self.model.fit(np.array(X)/255, np.array(y), batch_size=TRAINING_BATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if log_this_step else None)\n",
    "\n",
    "\n",
    "        if log_this_step:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "\n",
    "    def train_in_loop(self):\n",
    "        X = np.random.uniform(size=(1, IM_HEIGHT, IM_WIDTH, 3)).astype(np.float32)\n",
    "        y = np.random.uniform(size=(1, 3)).astype(np.float32)\n",
    "        with self.graph.as_default():\n",
    "            self.model.fit(X,y, verbose=False, batch_size=1)\n",
    "\n",
    "        self.training_initialized = True\n",
    "\n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                return\n",
    "            self.train()\n",
    "            time.sleep(0.01)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FPS = 60\n",
    "    # For stats\n",
    "    ep_rewards = [-200]\n",
    "\n",
    "    # For more repetitive results\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    tf.set_random_seed(1)\n",
    "\n",
    "    # Memory fraction, used mostly when trai8ning multiple agents\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)\n",
    "    backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))\n",
    "\n",
    "    # Create models folder\n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    # Create agent and environment\n",
    "    agent = DQNAgent()\n",
    "    env = CarEnv()\n",
    "\n",
    "\n",
    "    # Start training thread and wait for training to be initialized\n",
    "    trainer_thread = Thread(target=agent.train_in_loop, daemon=True)\n",
    "    trainer_thread.start()\n",
    "    while not agent.training_initialized:\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    # Initialize predictions - forst prediction takes longer as of initialization that has to be done\n",
    "    # It's better to do a first prediction then before we start iterating over episode steps\n",
    "    agent.get_qs(np.ones((env.im_height, env.im_width, 3)))\n",
    "\n",
    "    # Iterate over episodes\n",
    "    for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "        #try:\n",
    "\n",
    "            env.collision_hist = []\n",
    "\n",
    "            # Update tensorboard step every episode\n",
    "            agent.tensorboard.step = episode\n",
    "\n",
    "            # Restarting episode - reset episode reward and step number\n",
    "            episode_reward = 0\n",
    "            step = 1\n",
    "\n",
    "            # Reset environment and get initial state\n",
    "            current_state = env.reset()\n",
    "\n",
    "            # Reset flag and start iterating until episode ends\n",
    "            done = False\n",
    "            episode_start = time.time()\n",
    "\n",
    "            # Play for given number of seconds only\n",
    "            while True:\n",
    "\n",
    "                # This part stays mostly the same, the change is to query a model for Q values\n",
    "                if np.random.random() > epsilon:\n",
    "                    # Get action from Q table\n",
    "                    action = np.argmax(agent.get_qs(current_state))\n",
    "                else:\n",
    "                    # Get random action\n",
    "                    action = np.random.randint(0, 3)\n",
    "                    # This takes no time, so we add a delay matching 60 FPS (prediction above takes longer)\n",
    "                    time.sleep(1/FPS)\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Transform new continous state to new discrete state and count reward\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Every step we update replay memory\n",
    "                agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "\n",
    "                current_state = new_state\n",
    "                step += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # End of episode - destroy agents\n",
    "            for actor in env.actor_list:\n",
    "                actor.destroy()\n",
    "\n",
    "            # Append episode reward to a list and log stats (every given number of episodes)\n",
    "            ep_rewards.append(episode_reward)\n",
    "            if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "                average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "                min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "                max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "                agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "                # Save model, but only when min reward is greater or equal a set value\n",
    "                if min_reward >= MIN_REWARD:\n",
    "                    agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "            # Decay epsilon\n",
    "            if epsilon > MIN_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "                epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "\n",
    "    # Set termination flag for training thread and wait for it to finish\n",
    "    agent.terminate = True\n",
    "    trainer_thread.join()\n",
    "    agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
